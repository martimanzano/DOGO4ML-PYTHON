{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this demo is to showcase a scenario in which an hypothetical company wants to implement an AI system to predict which employees should be targeted for retention actions, based on various features of the employee. The AI system should be trustable, hence they want to use trustAI to assess the trustworthiness of the candidate classification models before their deployment and use for decision making.\n",
    "\n",
    "We use the synthetic dataset TED to simulate the scenario and the trustAI package to verify the model's trustworthiness. Unlike most datasets, each instance has also have an Explanation (E). Therefore, we will use the TED Cartesian Explainer as the explainer's framework, and the associated trustworthiness explainability metric included in the trustAI package.\n",
    "\n",
    "# Step 1: Defining the configuration file\n",
    "In this scenario, we are interested in the explainability trustworthiness dimension, as the company requires an AI system that provide explanations of why a retention action is needed to reduce the chances of an employee leaving the company. These rules are motivated by common scenarios, such as not getting a promotion in a while, not being paid competitively, receiving a disappointing evaluation, being a new employee in certain organizations with inherently high attrition, not having a salary that is consistent with positive evaluations, mid-career crisis, etc.\n",
    "\n",
    "The company is also interested in the performance trustworthiness dimension, as predicting accurately which employees should be targeted for retention (human resources) is equally important to predicting which ones should not be targeted for retention (budget restrictions).\n",
    "\n",
    "Based on this, we specify a configuration file based on metrics belonging to the two trustworthiness dimensions, and we specify the assessment method as a weighted average with equal weights for the two dimensions and the metrics that will be used.\n",
    "\n",
    "```yaml\n",
    "metrics:\n",
    "    - AccuracySKL\n",
    "    - PrecisionSKL:\n",
    "        multiclass_average: \"binary\"\n",
    "    - RecallSKL:\n",
    "        multiclass_average: \"binary\"\n",
    "    - F1SKL:\n",
    "        multiclass_average: \"binary\"\n",
    "    - ROCSKL:\n",
    "        multiclass_average: \"macro\"\n",
    "    - ExplanationsAccuracyTED:\n",
    "        explainer_path: \"demos/employees_retention/ted_explainer\"\n",
    "        explanations_path: \"demos/employees_retention/ted_explanations\"\n",
    "assessment_method:\n",
    "    WeightedAverage: # WEIGHTS FOR COMPUTING TRUST WITH THE WEIGHTED AVERAGED METHOD\n",
    "        performance-0.5:\n",
    "            AccuracySKL: 0.2\n",
    "            PrecisionSKL: 0.2\n",
    "            RecallSKL: 0.2\n",
    "            F1SKL: 0.2\n",
    "            ROCSKL: 0.2                \n",
    "        explainability-0.5:                \n",
    "            ExplanationsAccuracyTED: 1\n",
    "```\n",
    "\n",
    "# Step 2: Import relevant packages\n",
    "\n",
    "The first step will consist in importing the trustAI package, the classification model that we will use in the demo (RandomForestClassifier), the TED Cartesian Explainer that will be trained and used to explain the model's predictions and some supporting functions/modules, notably pandas for the dataset loading/manipulation, and train_test_split to partition the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from aix360.algorithms.ted.TED_Cartesian import TED_CartesianExplainer\n",
    "from TrustComputationService.TrustComputationService import TrustComputationService\n",
    "import pandas as pd\n",
    "from pickle import dump"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We force pandas to print every column of the dataset when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option(\"max_columns\")\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Load dataset and create train/test splits\n",
    "Below we define variables containing the file paths to the relevant files needed for this use case: the CSV dataset file and the previously defined configuration file.\n",
    "\n",
    "We then load the dataset using pandas, and we separate the features, the target, and the explanations (NOTE: Code partially extracted from https://github.com/Trusted-AI/AIX360/blob/master/aix360/datasets/ted_dataset.py). \n",
    "\n",
    "We also replace the values of the target column from -10, -11 to the more interpretable 0, 1 values. \n",
    "\n",
    "Finally, we create train/test splits for the features, target, and explainations, with 80%-20% train-test, correspondingly,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_path = 'demos/employees_retention/'\n",
    "dataset_path = demo_path + 'Retention.csv'\n",
    "configuration_path = demo_path + 'config_retention_corporate.yml'\n",
    "\n",
    "# Decompose the dataset into X, Y, E\n",
    "data = pd.read_csv(dataset_path)\n",
    "X = data.iloc[:,:-2]   # Choose all rows and all cols, except for the last 2 cols\n",
    "Y = data['Y']          # Choose col with header 'Y'\n",
    "E = data['E']          # Choose col with header 'E'\n",
    "\n",
    "# CLASSIFICATION COLUMN. [-10,-11] -> [0,1]\n",
    "Y.replace([-10,-11], [0,1], inplace=True)\n",
    "\n",
    "# Set up train/test split\n",
    "X_train, X_test, Y_train, Y_test, E_train, E_test = train_test_split(X, Y, E, test_size=0.3, random_state=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Create the classifier and the TED classifier\n",
    "Now we will create a random forest classifier and a TED-enhanced cartesian classifer, which will take the random forest classifier and will be trained taking into account the traininig explanations, i.e., the TED-enhanced classifier takes the training features and, for the target, it merges the actual targets (Y) and the explanations (E). Therefore, we will train a \"regular\" random forest classifier too, in order to evaluate the performance-related trustworthiness metrics on the Y targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN A RANDOM FOREST CLASSIFIER AND CREATE A TED-ENHANCED CLASSIFIER\n",
    "rf_classifier = RandomForestClassifier()\n",
    "ted = TED_CartesianExplainer(rf_classifier)\n",
    "\n",
    "rf_classifier_raw = RandomForestClassifier().fit(X_train, Y_train)\n",
    "ted.fit(X_train, Y_train, E_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Store required assets\n",
    "The explainability-related metric *ExplanationsAccuracyTED* requires two additional properties, a TED-enhanced classifier, and a dataset of explanations. Therefore, we will store such assets into the directory we have set in the configuration file. The \"ted_explainer\" property corresponds to the previously fitted TED-enhanced classifier, and the \"ted_explanations\" corresponds to the explanations we will test the classifier with, i.e., the test explanations. We store both using pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE EXPLAINER AND EXPLANATIONS TO THE DEMO DIRECTORY (OPTIONAL, ONLY NECESSARY IF THEY ARE NOT ALREADY PRESENT)\n",
    "with open(demo_path + 'ted_explainer', 'wb') as explainer_file:\n",
    "    dump(ted, explainer_file)\n",
    "with open(demo_path + 'ted_explanations', 'wb') as explanations_file:\n",
    "    dump(E_test, explanations_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Compute the trustworthiness\n",
    "Now we are ready to assess the model's trustworthiness with the *trustAI* package. For this, we instantiate a `TrustComputationService`, we call the `readTrustDefinition` method with the path to the configuration file we specified, and lastly we call the `computeTrust` function, passing the trained model and the test dataset (features and target) to evaluate the model's trustworthiness in such dataset. This function will return the trust assessment (in this case a float, as we use the weighted average assessment method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trust_instance = TrustComputationService()\n",
    "trust_instance.readTrustDefinition(configuration_path)\n",
    "print(trust_instance.computeTrust(rf_classifier_raw, X_test, Y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which results in a value of 0.99 of the trustworthiness indicator for the classification model that will conform the AI system.\n",
    "\n",
    "We can also print the complete trustworthiness assessment as a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trust_instance.getTrustAssessmentAsFormattedString())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```javascript\n",
    "{\n",
    "  \"name\": \"Trust\",\n",
    "  \"weighted_score\": 0.99,\n",
    "  \"children\": [\n",
    "    {\n",
    "      \"name\": \"performance\",\n",
    "      \"weight\": 0.5,\n",
    "      \"weighted_score\": 0.5,\n",
    "      \"raw_score\": 0.99,\n",
    "      \"children\": [\n",
    "        {\n",
    "          \"name\": \"AccuracySKL\",\n",
    "          \"weight\": 0.2,\n",
    "          \"weighted_score\": 0.2,\n",
    "          \"raw_score\": 0.99\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"PrecisionSKL\",\n",
    "          \"weight\": 0.2,\n",
    "          \"weighted_score\": 0.2,\n",
    "          \"raw_score\": 0.99\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"RecallSKL\",\n",
    "          \"weight\": 0.2,\n",
    "          \"weighted_score\": 0.2,\n",
    "          \"raw_score\": 1.0\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"F1SKL\",\n",
    "          \"weight\": 0.2,\n",
    "          \"weighted_score\": 0.2,\n",
    "          \"raw_score\": 0.99\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"ROCSKL\",\n",
    "          \"weight\": 0.2,\n",
    "          \"weighted_score\": 0.2,\n",
    "          \"raw_score\": 0.99\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"explainability\",\n",
    "      \"weight\": 0.5,\n",
    "      \"weighted_score\": 0.49,\n",
    "      \"raw_score\": 0.98,\n",
    "      \"children\": [\n",
    "        {\n",
    "          \"name\": \"ExplanationsAccuracyTED\",\n",
    "          \"weight\": 1,\n",
    "          \"weighted_score\": 0.98,\n",
    "          \"raw_score\": 0.98\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "This notebook has illustrated how easy it is to use the trustAI package to evaluate the trustworthiness of a classification model intended to be used as part of an AI system. In this case, the trustAI package has been used as part of a model building pipeline, obtaining a positive trustworthiness assessment (i.e., 0.99 out of 1). According to the trustability criteria of the ficticious company, the model would be deemed as acceptable without the need of undergoing modifications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env4ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9717086da0a9af0457c47e23af3ec726ed6d43f3f34da84c0887679a92baa5a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

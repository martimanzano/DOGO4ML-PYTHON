{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this demo is to showcase a scenario in which a ficticious open source community wants to implement an AI system to assist the pull request analysis before they are merged or rejected. New pull requests should be classified as potentially acceptable or not, based on various features of the pull request. The AI system should be trustable, hence they want to use TrustML to assess the trustworthiness of the candidate classification models before their deployment.\n",
    "\n",
    "We use a dataset from the MSR 2020 conference (https://github.com/zhangxunhui/new_pullreq_msr2020) to simulate the scenario and the TrustML package to evaluate and assess the model's trustworthiness. In this case, the dataset is considerably large (2.2 GB) and needs some preprocessing before it may be used to build a classification model on. The applied preprocessing is shown separately in the file \"original_dataset_preprocessing.py\".\n",
    "\n",
    "# Step 1: Defining the configuration file\n",
    "In this scenario, the open source community is interested in having an AI model complying with several trustworthiness criteria. In decreasing order of importance for them, the AI system should comply with:\n",
    "1. Performance: It is important to correctly predict the potential acceptable pull requests.\n",
    "2. Explainability: The community will need models that provide explanations to them, in order to enable the comprehension of how the AI system predicts the potential acceptability of the pull request.\n",
    "3. Uncertainty: Knowing how uncertain the prediction is might influence the banker to (not) assume the risks of merging the pull requests.\n",
    "3. Fairness (equal importance): Ethical aspects to prevent scandal. Protected attributes: gender of the pull request submitter.\n",
    "\n",
    "Based on this, we specify a configuration file based on metrics belonging to the listed trustworthiness dimensions, and we specify the assessment method as a weighted average with equal weights for the two dimensions and the metrics that will be used.\n",
    "\n",
    "This is the content of the configuration file we will use:\n",
    "\n",
    "```yaml\n",
    "metrics:\n",
    "    - AccuracySKL\n",
    "    - PrecisionSKL:\n",
    "        multiclass_average: \"binary\"\n",
    "    - RecallSKL:\n",
    "        multiclass_average: \"binary\"\n",
    "    - PPercentageSKL:\n",
    "        protected_attributes: [contrib_gender]\n",
    "        positive_class: 1\n",
    "    - EqualOpportunitySKL:\n",
    "        protected_attributes: [contrib_gender]\n",
    "        positive_class: 1\n",
    "    - FaithfulnessLIMESKL:\n",
    "        explainer_path: \"demos/pull_request/lime_explainer\"\n",
    "    - InvertedExpectedCalibrationSKL\n",
    "    - InvertedBrierSKL\n",
    "assessment_method:\n",
    "    WeightedAverage:\n",
    "        performance-0.5:\n",
    "            AccuracySKL: 0.7\n",
    "            PrecisionSKL: 0.15\n",
    "            RecallSKL: 0.15\n",
    "        uncertainty-0.15:                  \n",
    "            InvertedBrierSKL: 0.5\n",
    "            InvertedExpectedCalibrationSKL: 0.5\n",
    "        explainability-0.2:                \n",
    "            FaithfulnessLIMESKL: 1\n",
    "        fairness-0.15:             \n",
    "            PPercentageSKL: 0.5\n",
    "            EqualOpportunitySKL: 0.5\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Import relevant packages\n",
    "The first step will consist in importing the TrustML package, the classification model that we will use in the demo (RandomForestClassifier) and some supporting functions/modules, notably pandas for the dataset loading/manipulation, LimeTabularExplainer to train and evaluate an explainer for the model, and train_test_split to partition the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "from dill import dump\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "from trustML.computation import TrustComputation\n",
    "\n",
    "pd.reset_option(\"max_columns\")\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Load dataset and create train/test splits\n",
    "Now we load the CSV file, extract the target column (i.e., if the pull request was merged or not), and split the dataset into training and test, with a 80%-20% proportion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_path = 'demos/pull_request/'\n",
    "path_configuration = demo_path + 'config_pull_request.yml'\n",
    "file_dataset = demo_path + 'new_pullreq-red.csv'\n",
    "\n",
    "# Load the data\n",
    "dataset = pd.read_csv(file_dataset, sep=\",\", header=0)\n",
    "dataset.head(0)\n",
    "\n",
    "# Extract target column\n",
    "Y = dataset[\"merged_or_not\"]\n",
    "Y.describe()\n",
    "\n",
    "# Decompose the dataset: Training and test split and drop target\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(dataset.drop(columns=[\"merged_or_not\"]), Y, test_size=0.2, stratify=dataset.merged_or_not, random_state=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Create and train the classifier\n",
    "We will now train a random forest classifier on the training set, as well as a lime tabular explainer that will be able to provide prediction's explanations based on the random forest's output. This explainer will be also assessed for trustworthiness as part of the explainability-related metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TRAIN A RANDOM FOREST CLASSIFIER\n",
    "rf_classifier = RandomForestClassifier()\n",
    "rf_classifier.fit(X_train, Y_train)\n",
    "\n",
    "# TRAIN A LIME TABULAR EXPLAINER\n",
    "lime_explainer = LimeTabularExplainer(X_train.values, feature_names=X_train.columns.values, class_names=[1,0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Store required assets\n",
    "The explainability-related metric *FaithfulnessLIMESKL* requires an additional properties, a LIME tabular explainer. Therefore, we will store such asset into the directory we have set in the configuration file. The \"explainer_path\" property corresponds to the previously fitted lime tabular explainer. We store it using dill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE EXPLAINER TO THE DEMO DIRECTORY (OPTIONAL, ONLY NECESSARY IF THEY ARE NOT ALREADY PRESENT)\n",
    "with open(demo_path + 'lime_explainer', 'wb') as explainer_file:\n",
    "    dump(lime_explainer, explainer_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Compute the trustworthiness\n",
    "Once trained, we will assess its trustworthiness with the TrustML package. For this, we instantiate a TrustComputation object, we call the load_trust_definition method with the path to the configuration file we specified, and lastly we call the compute_trust function, passing the trained model and the test dataset (features and target) to evaluate the model's trustworthiness in such dataset. This function stores the trust assessment as a JSON-formatted string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing p-percentage vector...\n",
      "Computing equal opportunity metric...\n",
      "Computing faithfulness metric with LIME...\n",
      "Case 5/8617\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m trust_pr \u001b[39m=\u001b[39m TrustComputation()\n\u001b[0;32m      3\u001b[0m trust_pr\u001b[39m.\u001b[39mload_trust_definition(config_path\u001b[39m=\u001b[39mpath_configuration)\n\u001b[1;32m----> 4\u001b[0m trust_pr\u001b[39m.\u001b[39;49mcompute_trust(trained_model\u001b[39m=\u001b[39;49mrf_classifier, data_x\u001b[39m=\u001b[39;49mX_test, data_y\u001b[39m=\u001b[39;49mY_test)\n",
      "File \u001b[1;32mD:\\DOGO4ML-PYTHON\\src\\trustML\\computation.py:30\u001b[0m, in \u001b[0;36mTrustComputation.compute_trust\u001b[1;34m(self, trained_model, data_x, data_y)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_trust\u001b[39m(\u001b[39mself\u001b[39m, trained_model, data_x, data_y):\n\u001b[0;32m     20\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Performs the metrics' assessments, followed by the trust assessment\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39m    based on such metrics and the specified assessment method and its parameters.\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[39m    Leverages this process to the TWI class.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39m        data_y (pandas dataset): target values of the dataset to evaluate\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrust\u001b[39m.\u001b[39;49massess(trained_model, data_x, data_y)\n",
      "File \u001b[1;32mD:\\DOGO4ML-PYTHON\\src\\trustML\\trust.py:29\u001b[0m, in \u001b[0;36mTrust.assess\u001b[1;34m(self, trained_model, data_x, data_y)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_y \u001b[39m=\u001b[39m data_y\n\u001b[0;32m     28\u001b[0m \u001b[39mfor\u001b[39;00m metric \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetrics:\n\u001b[1;32m---> 29\u001b[0m     metric\u001b[39m.\u001b[39;49massess(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrained_model, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_y)\n\u001b[0;32m     31\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrust_dict, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrust_JSON \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39massessment_method\u001b[39m.\u001b[39massess()\n",
      "File \u001b[1;32mD:\\DOGO4ML-PYTHON\\src\\trustML\\metrics\\faithfulnessLIME.py:38\u001b[0m, in \u001b[0;36mFaithfulnessLIMESKL.assess\u001b[1;34m(self, trained_model, data_x, data_y)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(ncases):\n\u001b[0;32m     37\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCase \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(ncases), end\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 38\u001b[0m     explanation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexplainer\u001b[39m.\u001b[39;49mexplain_instance(\n\u001b[0;32m     39\u001b[0m         data_x\u001b[39m.\u001b[39;49mvalues[i], trained_model\u001b[39m.\u001b[39;49mpredict_proba, num_features\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, top_labels\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, num_samples\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n\u001b[0;32m     40\u001b[0m     local_explanation \u001b[39m=\u001b[39m explanation\u001b[39m.\u001b[39mlocal_exp[\u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(explanation\u001b[39m.\u001b[39mlocal_exp))]\u001b[39m#explanation.local_exp[predicted_class]\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     x \u001b[39m=\u001b[39m data_x\u001b[39m.\u001b[39mvalues[i]\n",
      "File \u001b[1;32md:\\DOGO4ML-PYTHON\\env4ml\\lib\\site-packages\\lime\\lime_tabular.py:452\u001b[0m, in \u001b[0;36mLimeTabularExplainer.explain_instance\u001b[1;34m(self, data_row, predict_fn, labels, top_labels, num_features, num_samples, distance_metric, model_regressor)\u001b[0m\n\u001b[0;32m    448\u001b[0m     labels \u001b[39m=\u001b[39m [\u001b[39m0\u001b[39m]\n\u001b[0;32m    449\u001b[0m \u001b[39mfor\u001b[39;00m label \u001b[39min\u001b[39;00m labels:\n\u001b[0;32m    450\u001b[0m     (ret_exp\u001b[39m.\u001b[39mintercept[label],\n\u001b[0;32m    451\u001b[0m      ret_exp\u001b[39m.\u001b[39mlocal_exp[label],\n\u001b[1;32m--> 452\u001b[0m      ret_exp\u001b[39m.\u001b[39mscore, ret_exp\u001b[39m.\u001b[39mlocal_pred) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase\u001b[39m.\u001b[39;49mexplain_instance_with_data(\n\u001b[0;32m    453\u001b[0m             scaled_data,\n\u001b[0;32m    454\u001b[0m             yss,\n\u001b[0;32m    455\u001b[0m             distances,\n\u001b[0;32m    456\u001b[0m             label,\n\u001b[0;32m    457\u001b[0m             num_features,\n\u001b[0;32m    458\u001b[0m             model_regressor\u001b[39m=\u001b[39;49mmodel_regressor,\n\u001b[0;32m    459\u001b[0m             feature_selection\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeature_selection)\n\u001b[0;32m    461\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mregression\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    462\u001b[0m     ret_exp\u001b[39m.\u001b[39mintercept[\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m ret_exp\u001b[39m.\u001b[39mintercept[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32md:\\DOGO4ML-PYTHON\\env4ml\\lib\\site-packages\\lime\\lime_base.py:183\u001b[0m, in \u001b[0;36mLimeBase.explain_instance_with_data\u001b[1;34m(self, neighborhood_data, neighborhood_labels, distances, label, num_features, feature_selection, model_regressor)\u001b[0m\n\u001b[0;32m    181\u001b[0m weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel_fn(distances)\n\u001b[0;32m    182\u001b[0m labels_column \u001b[39m=\u001b[39m neighborhood_labels[:, label]\n\u001b[1;32m--> 183\u001b[0m used_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeature_selection(neighborhood_data,\n\u001b[0;32m    184\u001b[0m                                        labels_column,\n\u001b[0;32m    185\u001b[0m                                        weights,\n\u001b[0;32m    186\u001b[0m                                        num_features,\n\u001b[0;32m    187\u001b[0m                                        feature_selection)\n\u001b[0;32m    188\u001b[0m \u001b[39mif\u001b[39;00m model_regressor \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    189\u001b[0m     model_regressor \u001b[39m=\u001b[39m Ridge(alpha\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, fit_intercept\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    190\u001b[0m                             random_state\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrandom_state)\n",
      "File \u001b[1;32md:\\DOGO4ML-PYTHON\\env4ml\\lib\\site-packages\\lime\\lime_base.py:134\u001b[0m, in \u001b[0;36mLimeBase.feature_selection\u001b[1;34m(self, data, labels, weights, num_features, method)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    133\u001b[0m     n_method \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mhighest_weights\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 134\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeature_selection(data, labels, weights,\n\u001b[0;32m    135\u001b[0m                               num_features, n_method)\n",
      "File \u001b[1;32md:\\DOGO4ML-PYTHON\\env4ml\\lib\\site-packages\\lime\\lime_base.py:76\u001b[0m, in \u001b[0;36mLimeBase.feature_selection\u001b[1;34m(self, data, labels, weights, num_features, method)\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray(\u001b[39mrange\u001b[39m(data\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]))\n\u001b[0;32m     75\u001b[0m \u001b[39melif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mforward_selection\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m---> 76\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_selection(data, labels, weights, num_features)\n\u001b[0;32m     77\u001b[0m \u001b[39melif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mhighest_weights\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m     78\u001b[0m     clf \u001b[39m=\u001b[39m Ridge(alpha\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m, fit_intercept\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     79\u001b[0m                 random_state\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrandom_state)\n",
      "File \u001b[1;32md:\\DOGO4ML-PYTHON\\env4ml\\lib\\site-packages\\lime\\lime_base.py:59\u001b[0m, in \u001b[0;36mLimeBase.forward_selection\u001b[1;34m(self, data, labels, weights, num_features)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39mif\u001b[39;00m feature \u001b[39min\u001b[39;00m used_features:\n\u001b[0;32m     58\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m clf\u001b[39m.\u001b[39;49mfit(data[:, used_features \u001b[39m+\u001b[39;49m [feature]], labels,\n\u001b[0;32m     60\u001b[0m         sample_weight\u001b[39m=\u001b[39;49mweights)\n\u001b[0;32m     61\u001b[0m score \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mscore(data[:, used_features \u001b[39m+\u001b[39m [feature]],\n\u001b[0;32m     62\u001b[0m                   labels,\n\u001b[0;32m     63\u001b[0m                   sample_weight\u001b[39m=\u001b[39mweights)\n\u001b[0;32m     64\u001b[0m \u001b[39mif\u001b[39;00m score \u001b[39m>\u001b[39m max_:\n",
      "File \u001b[1;32md:\\DOGO4ML-PYTHON\\env4ml\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py:762\u001b[0m, in \u001b[0;36mRidge.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    743\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    744\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Fit Ridge regression model.\u001b[39;00m\n\u001b[0;32m    745\u001b[0m \n\u001b[0;32m    746\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    760\u001b[0m \u001b[39m    self : returns an instance of self.\u001b[39;00m\n\u001b[0;32m    761\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 762\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(X, y, sample_weight\u001b[39m=\u001b[39;49msample_weight)\n",
      "File \u001b[1;32md:\\DOGO4ML-PYTHON\\env4ml\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py:572\u001b[0m, in \u001b[0;36m_BaseRidge.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    568\u001b[0m     sample_weight \u001b[39m=\u001b[39m _check_sample_weight(sample_weight, X,\n\u001b[0;32m    569\u001b[0m                                          dtype\u001b[39m=\u001b[39mX\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m    571\u001b[0m \u001b[39m# when X is sparse we only remove offset from y\u001b[39;00m\n\u001b[1;32m--> 572\u001b[0m X, y, X_offset, y_offset, X_scale \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_preprocess_data(\n\u001b[0;32m    573\u001b[0m     X, y, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_intercept, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnormalize, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcopy_X,\n\u001b[0;32m    574\u001b[0m     sample_weight\u001b[39m=\u001b[39;49msample_weight, return_mean\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    576\u001b[0m \u001b[39mif\u001b[39;00m solver \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39msag\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m sparse\u001b[39m.\u001b[39missparse(X) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_intercept:\n\u001b[0;32m    577\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoef_, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_iter_, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintercept_ \u001b[39m=\u001b[39m _ridge_regression(\n\u001b[0;32m    578\u001b[0m         X, y, alpha\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39malpha, sample_weight\u001b[39m=\u001b[39msample_weight,\n\u001b[0;32m    579\u001b[0m         max_iter\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_iter, tol\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtol, solver\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msag\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m    580\u001b[0m         random_state\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrandom_state, return_n_iter\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    581\u001b[0m         return_intercept\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32md:\\DOGO4ML-PYTHON\\env4ml\\lib\\site-packages\\sklearn\\linear_model\\_base.py:130\u001b[0m, in \u001b[0;36m_preprocess_data\u001b[1;34m(X, y, fit_intercept, normalize, copy, sample_weight, return_mean, check_input)\u001b[0m\n\u001b[0;32m    127\u001b[0m     sample_weight \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(sample_weight)\n\u001b[0;32m    129\u001b[0m \u001b[39mif\u001b[39;00m check_input:\n\u001b[1;32m--> 130\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, copy\u001b[39m=\u001b[39;49mcopy, accept_sparse\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    131\u001b[0m                     dtype\u001b[39m=\u001b[39;49mFLOAT_DTYPES)\n\u001b[0;32m    132\u001b[0m \u001b[39melif\u001b[39;00m copy:\n\u001b[0;32m    133\u001b[0m     \u001b[39mif\u001b[39;00m sp\u001b[39m.\u001b[39missparse(X):\n",
      "File \u001b[1;32md:\\DOGO4ML-PYTHON\\env4ml\\lib\\site-packages\\sklearn\\utils\\validation.py:63\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m extra_args \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(args) \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(all_args)\n\u001b[0;32m     62\u001b[0m \u001b[39mif\u001b[39;00m extra_args \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[39m# extra_args > 0\u001b[39;00m\n\u001b[0;32m     66\u001b[0m args_msg \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(name, arg)\n\u001b[0;32m     67\u001b[0m             \u001b[39mfor\u001b[39;00m name, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(kwonly_args[:extra_args],\n\u001b[0;32m     68\u001b[0m                                  args[\u001b[39m-\u001b[39mextra_args:])]\n",
      "File \u001b[1;32md:\\DOGO4ML-PYTHON\\env4ml\\lib\\site-packages\\sklearn\\utils\\validation.py:683\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    677\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mFound array with \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m feature(s) (shape=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m) while\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    678\u001b[0m                          \u001b[39m\"\u001b[39m\u001b[39m a minimum of \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m is required\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    679\u001b[0m                          \u001b[39m%\u001b[39m (n_features, array\u001b[39m.\u001b[39mshape, ensure_min_features,\n\u001b[0;32m    680\u001b[0m                             context))\n\u001b[0;32m    682\u001b[0m \u001b[39mif\u001b[39;00m copy \u001b[39mand\u001b[39;00m np\u001b[39m.\u001b[39mmay_share_memory(array, array_orig):\n\u001b[1;32m--> 683\u001b[0m     array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49marray(array, dtype\u001b[39m=\u001b[39;49mdtype, order\u001b[39m=\u001b[39;49morder)\n\u001b[0;32m    685\u001b[0m \u001b[39mreturn\u001b[39;00m array\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TRUST STUFF\n",
    "trust_pr = TrustComputation()\n",
    "trust_pr.load_trust_definition(config_path=path_configuration)\n",
    "trust_pr.compute_trust(trained_model=rf_classifier, data_x=X_test, data_y=Y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can print the complete trustworthiness assessment as a JSON-formatted string using the getTrustworthinessScore function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trust_pr.get_trust_as_JSON())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which results in the following output:\n",
    "\n",
    "```javascript\n",
    "{\n",
    "  \"name\": \"Trust\",\n",
    "  \"weighted_score\": 0.8,\n",
    "  \"children\": [\n",
    "    {\n",
    "      \"name\": \"performance\",\n",
    "      \"weight\": 0.5,\n",
    "      \"weighted_score\": 0.39,\n",
    "      \"raw_score\": 0.78,\n",
    "      \"children\": [\n",
    "        {\n",
    "          \"name\": \"AccuracySKL\",\n",
    "          \"weight\": 0.7,\n",
    "          \"weighted_score\": 0.55,\n",
    "          \"raw_score\": 0.78\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"PrecisionSKL\",\n",
    "          \"weight\": 0.15,\n",
    "          \"weighted_score\": 0.12,\n",
    "          \"raw_score\": 0.78\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"RecallSKL\",\n",
    "          \"weight\": 0.15,\n",
    "          \"weighted_score\": 0.12,\n",
    "          \"raw_score\": 0.79\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"uncertainty\",\n",
    "      \"weight\": 0.15,\n",
    "      \"weighted_score\": 0.1,\n",
    "      \"raw_score\": 0.65,\n",
    "      \"children\": [\n",
    "        {\n",
    "          \"name\": \"InvertedBrierSKL\",\n",
    "          \"weight\": 0.5,\n",
    "          \"weighted_score\": 0.2,\n",
    "          \"raw_score\": 0.41\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"InvertedExpectedCalibrationSKL\",\n",
    "          \"weight\": 0.5,\n",
    "          \"weighted_score\": 0.45,\n",
    "          \"raw_score\": 0.9\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"fairness\",\n",
    "      \"weight\": 0.35,\n",
    "      \"weighted_score\": 0.32,\n",
    "      \"raw_score\": 0.9,\n",
    "      \"children\": [\n",
    "        {\n",
    "          \"name\": \"PPercentageSKL\",\n",
    "          \"weight\": 0.5,\n",
    "          \"weighted_score\": 0.46,\n",
    "          \"raw_score\": 0.92\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"EqualOpportunitySKL\",\n",
    "          \"weight\": 0.5,\n",
    "          \"weighted_score\": 0.44,\n",
    "          \"raw_score\": 0.88\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "Which results in a value of 0.8 of the trustworthiness indicator for the classification model that will conform the AI system.\n",
    "\n",
    "This notebook has illustrated how easy it is to use the TrustML package to evaluate the trustworthiness of a classification model intended to be used as part of an AI system. In this case, the TrustML package has been used as part of a model building pipeline, obtaining a trustworthiness assessment of 0.8 (out of 1). According to the trustability criteria of the ficticious open source community, the model would be deemed acceptable, as their acceptance threshold is indeed 0.8."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env4ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9717086da0a9af0457c47e23af3ec726ed6d43f3f34da84c0887679a92baa5a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this demo is to showcase a scenario in which a ficticious open source community wants to implement an AI system to assist the pull request analysis before they are merged or rejected. New pull requests should be classified as potentially acceptable or not, based on various features of the pull request. The AI system should be trustable, hence they want to use TrustML to assess the trustworthiness of the candidate classification models before their deployment.\n",
    "\n",
    "We use a dataset from the MSR 2020 conference (https://github.com/zhangxunhui/new_pullreq_msr2020) to simulate the scenario and the TrustML package to evaluate and assess the model's trustworthiness. In this case, the dataset is considerably large (2.2 GB) and needs some preprocessing before it may be used to build a classification model on. The applied preprocessing is shown separately in the file \"original_dataset_preprocessing.py\".\n",
    "\n",
    "# Step 1: Defining the configuration file\n",
    "In this scenario, the open source community is interested in having an AI model complying with several trustworthiness criteria. In decreasing order of importance for them, the AI system should comply with:\n",
    "1. Performance: It is important to correctly predict the potential acceptable pull requests.\n",
    "2. Explainability: The community will need models that provide explanations to them, in order to enable the comprehension of how the AI system predicts the potential acceptability of the pull request.\n",
    "3. Uncertainty: Knowing how uncertain the prediction is might influence the banker to (not) assume the risks of merging the pull requests.\n",
    "3. Fairness (equal importance): Ethical aspects to prevent scandal. Protected attributes: gender of the pull request submitter.\n",
    "\n",
    "Based on this, we specify a configuration file based on metrics belonging to the listed trustworthiness dimensions, and we specify the assessment method as a weighted average with equal weights for the two dimensions and the metrics that will be used.\n",
    "\n",
    "This is the content of the configuration file we will use:\n",
    "\n",
    "```yaml\n",
    "metrics:\n",
    "    - AccuracySKL\n",
    "    - PrecisionSKL:\n",
    "        multiclass_average: \"binary\"\n",
    "    - RecallSKL:\n",
    "        multiclass_average: \"binary\"\n",
    "    - PPercentageSKL:\n",
    "        protected_attributes: [contrib_gender]\n",
    "        positive_class: 1\n",
    "    - EqualOpportunitySKL:\n",
    "        protected_attributes: [contrib_gender]\n",
    "        positive_class: 1\n",
    "    - FaithfulnessLIMESKL:\n",
    "        explainer_path: \"demos/pull_request/lime_explainer\"\n",
    "    - InvertedExpectedCalibrationSKL\n",
    "    - InvertedBrierSKL\n",
    "assessment_method:\n",
    "    WeightedAverage:\n",
    "        performance-0.5:\n",
    "            AccuracySKL: 0.7\n",
    "            PrecisionSKL: 0.15\n",
    "            RecallSKL: 0.15\n",
    "        uncertainty-0.15:                  \n",
    "            InvertedBrierSKL: 0.5\n",
    "            InvertedExpectedCalibrationSKL: 0.5\n",
    "        explainability-0.2:                \n",
    "            FaithfulnessLIMESKL: 1\n",
    "        fairness-0.15:             \n",
    "            PPercentageSKL: 0.5\n",
    "            EqualOpportunitySKL: 0.5\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Import relevant packages\n",
    "The first step will consist in importing the TrustML package, the classification model that we will use in the demo (RandomForestClassifier) and some supporting functions/modules, notably pandas for the dataset loading/manipulation, LimeTabularExplainer to train and evaluate an explainer for the model, and train_test_split to partition the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "from dill import dump\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "from trustML.computation import TrustComputation\n",
    "\n",
    "pd.reset_option(\"max_columns\")\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Load dataset and create train/test splits\n",
    "Now we load the CSV file, extract the target column (i.e., if the pull request was merged or not), and split the dataset into training and test, with a 80%-20% proportion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_path = 'demos/pull_request/'\n",
    "path_configuration = demo_path + 'config_pull_request.yml'\n",
    "file_dataset = demo_path + 'new_pullreq-red.csv'\n",
    "\n",
    "# Load the data\n",
    "dataset = pd.read_csv(file_dataset, sep=\",\", header=0)\n",
    "dataset.head(0)\n",
    "\n",
    "# Extract target column\n",
    "Y = dataset[\"merged_or_not\"]\n",
    "Y.describe()\n",
    "\n",
    "# Decompose the dataset: Training and test split and drop target\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(dataset.drop(columns=[\"merged_or_not\"]), Y, test_size=0.2, stratify=dataset.merged_or_not, random_state=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Create and train the classifier\n",
    "We will now train a random forest classifier on the training set, as well as a lime tabular explainer that will be able to provide prediction's explanations based on the random forest's output. This explainer will be also assessed for trustworthiness as part of the explainability-related metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TRAIN A RANDOM FOREST CLASSIFIER\n",
    "rf_classifier = RandomForestClassifier()\n",
    "rf_classifier.fit(X_train, Y_train)\n",
    "\n",
    "# TRAIN A LIME TABULAR EXPLAINER\n",
    "lime_explainer = LimeTabularExplainer(X_train.values, feature_names=X_train.columns.values, class_names=[1,0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Store required assets\n",
    "The explainability-related metric *FaithfulnessLIMESKL* requires an additional properties, a LIME tabular explainer. Therefore, we will store such asset into the directory we have set in the configuration file. The \"explainer_path\" property corresponds to the previously fitted lime tabular explainer. We store it using dill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE EXPLAINER TO THE DEMO DIRECTORY (OPTIONAL, ONLY NECESSARY IF THEY ARE NOT ALREADY PRESENT)\n",
    "with open(demo_path + 'lime_explainer', 'wb') as explainer_file:\n",
    "    dump(lime_explainer, explainer_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Compute the trustworthiness\n",
    "Once trained, we will assess its trustworthiness with the TrustML package. For this, we instantiate a TrustComputation object, we call the load_trust_definition method with the path to the configuration file we specified, and lastly we call the compute_trust function, passing the trained model and the test dataset (features and target) to evaluate the model's trustworthiness in such dataset. This function stores the trust assessment as a JSON-formatted string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRUST STUFF\n",
    "trust_pr = TrustComputation()\n",
    "trust_pr.load_trust_definition(config_path=path_configuration)\n",
    "trust_pr.compute_trust(trained_model=rf_classifier, data_x=X_test, data_y=Y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can print the complete trustworthiness assessment as a JSON-formatted string using the get_trust_as_JSON function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trust_pr.get_trust_as_JSON())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which results in the following output:\n",
    "\n",
    "```javascript\n",
    "{\n",
    "  \"name\": \"Trust\",\n",
    "  \"weighted_score\": 0.71,\n",
    "  \"children\": [\n",
    "    {\n",
    "      \"name\": \"performance\",\n",
    "      \"weight\": 0.5,\n",
    "      \"weighted_score\": 0.37,\n",
    "      \"raw_score\": 0.75,\n",
    "      \"children\": [\n",
    "        {\n",
    "          \"name\": \"AccuracySKL\",\n",
    "          \"weight\": 0.7,\n",
    "          \"weighted_score\": 0.52,\n",
    "          \"raw_score\": 0.74\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"PrecisionSKL\",\n",
    "          \"weight\": 0.15,\n",
    "          \"weighted_score\": 0.11,\n",
    "          \"raw_score\": 0.73\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"RecallSKL\",\n",
    "          \"weight\": 0.15,\n",
    "          \"weighted_score\": 0.11,\n",
    "          \"raw_score\": 0.76\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"uncertainty\",\n",
    "      \"weight\": 0.15,\n",
    "      \"weighted_score\": 0.1,\n",
    "      \"raw_score\": 0.67,\n",
    "      \"children\": [\n",
    "        {\n",
    "          \"name\": \"InvertedBrierSKL\",\n",
    "          \"weight\": 0.5,\n",
    "          \"weighted_score\": 0.2,\n",
    "          \"raw_score\": 0.41\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"InvertedExpectedCalibrationSKL\",\n",
    "          \"weight\": 0.5,\n",
    "          \"weighted_score\": 0.47,\n",
    "          \"raw_score\": 0.94\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"explainability\",\n",
    "      \"weight\": 0.2,\n",
    "      \"weighted_score\": 0.1,\n",
    "      \"raw_score\": 0.52,\n",
    "      \"children\": [\n",
    "        {\n",
    "          \"name\": \"FaithfulnessLIMESKL\",\n",
    "          \"weight\": 1,\n",
    "          \"weighted_score\": 0.52,\n",
    "          \"raw_score\": 0.52\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"fairness\",\n",
    "      \"weight\": 0.15,\n",
    "      \"weighted_score\": 0.14,\n",
    "      \"raw_score\": 0.9,\n",
    "      \"children\": [\n",
    "        {\n",
    "          \"name\": \"PPercentageSKL\",\n",
    "          \"weight\": 0.5,\n",
    "          \"weighted_score\": 0.44,\n",
    "          \"raw_score\": 0.88\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"EqualOpportunitySKL\",\n",
    "          \"weight\": 0.5,\n",
    "          \"weighted_score\": 0.46,\n",
    "          \"raw_score\": 0.93\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also generate a graphical report in PDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trust_pr.generate_trust_PDF(save_path=demo_path + \"reportPR.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which would generate a PDF an excerpt of which is shown in the following image:\n",
    "\n",
    "![Report excerpt](excerpt_report.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "Which results in a value of 0.71 of the trustworthiness indicator for the classification model that will conform the AI system.\n",
    "\n",
    "This notebook has illustrated how easy it is to use the TrustML package to evaluate the trustworthiness of a classification model intended to be used as part of an AI system. In this case, the TrustML package has been used as part of a model building pipeline, obtaining a trustworthiness assessment of 0.8 (out of 1). According to the trustability criteria of the ficticious open source community, the model would be deemed acceptable, as their acceptance threshold in this case is of 0.7."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env4ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9717086da0a9af0457c47e23af3ec726ed6d43f3f34da84c0887679a92baa5a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this demo is to showcase a scenario in which a ficticious open source community wants to implement an AI system to assist the pull request analysis before they are merged or rejected. New pull requests should be classified as potentially acceptable or not, based on various features of the pull request. The AI system should be trustable, hence they want to use TrustML to assess the trustworthiness of the candidate classification models before their deployment.\n",
    "\n",
    "We use a dataset from the MSR 2020 conference (https://github.com/zhangxunhui/new_pullreq_msr2020) to simulate the scenario and the TrustML package to evaluate and assess the model's trustworthiness. In this case, the dataset is considerably large (2.2 GB) and needs some preprocessing before it may be used to build a classification model on. The applied preprocessing is shown separately in the file \"original_dataset_preprocessing.py\".\n",
    "\n",
    "# Step 1: Defining the configuration file\n",
    "In this scenario, the open source community is interested in having an AI model complying with several trustworthiness criteria. In decreasing order of importance for them, the AI system should comply with:\n",
    "1. Performance: It is important to correctly predict the potential acceptable pull requests.\n",
    "2. Explainability: The community will need models that provide explanations to them, in order to enable the comprehension of how the AI system predicts the potential acceptability of the pull request.\n",
    "3. Uncertainty: Knowing how uncertain the prediction is might influence the banker to (not) assume the risks of merging the pull requests.\n",
    "3. Fairness (equal importance): Ethical aspects to prevent scandal. Protected attributes: gender of the pull request submitter.\n",
    "\n",
    "Based on this, we specify a configuration file based on metrics belonging to the listed trustworthiness dimensions, and we specify the assessment method as a weighted average with equal weights for the two dimensions and the metrics that will be used.\n",
    "\n",
    "This is the content of the configuration file we will use:\n",
    "\n",
    "```yaml\n",
    "metrics:\n",
    "    - AccuracySKL\n",
    "    - PrecisionSKL:\n",
    "        multiclass_average: \"binary\"\n",
    "    - RecallSKL:\n",
    "        multiclass_average: \"binary\"\n",
    "    - PPercentageSKL:\n",
    "        protected_attributes: [contrib_gender]\n",
    "        positive_class: 1\n",
    "    - EqualOpportunitySKL:\n",
    "        protected_attributes: [contrib_gender]\n",
    "        positive_class: 1\n",
    "    - FaithfulnessLIMESKL:\n",
    "        explainer_path: \"demos/pull_request/lime_explainer\"\n",
    "    - InvertedExpectedCalibrationSKL\n",
    "    - InvertedBrierSKL\n",
    "assessment_method:\n",
    "    WeightedAverage:\n",
    "        performance-0.5:\n",
    "            AccuracySKL: 0.7\n",
    "            PrecisionSKL: 0.15\n",
    "            RecallSKL: 0.15\n",
    "        uncertainty-0.15:                  \n",
    "            InvertedBrierSKL: 0.5\n",
    "            InvertedExpectedCalibrationSKL: 0.5\n",
    "        explainability-0.2:                \n",
    "            FaithfulnessLIMESKL: 1\n",
    "        fairness-0.15:             \n",
    "            PPercentageSKL: 0.5\n",
    "            EqualOpportunitySKL: 0.5\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Import relevant packages\n",
    "The first step will consist in importing the TrustML package, the classification model that we will use in the demo (RandomForestClassifier) and some supporting functions/modules, notably pandas for the dataset loading/manipulation, LimeTabularExplainer to train and evaluate an explainer for the model, and train_test_split to partition the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "from dill import dump\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "from trustML.TrustworthinessComputationFacane.TrustworthinessComputationFacane import TrustworthinessComputationFacane\n",
    "\n",
    "pd.reset_option(\"max_columns\")\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Load dataset and create train/test splits\n",
    "Now we load the CSV file, extract the target column (i.e., if the pull request was merged or not), and split the dataset into training and test, with a 80%-20% proportion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "demo_path = './'\n",
    "path_configuration = demo_path + 'config_pull_request.yml'\n",
    "file_dataset = demo_path + 'new_pullreq-red.csv'\n",
    "\n",
    "# Load the data\n",
    "dataset = pd.read_csv(file_dataset, sep=\",\", header=0)\n",
    "dataset.head(0)\n",
    "\n",
    "# Extract target column\n",
    "Y = dataset[\"merged_or_not\"]\n",
    "Y.describe()\n",
    "\n",
    "# Decompose the dataset: Training and test split and drop target\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(dataset.drop(columns=[\"merged_or_not\"]), Y, test_size=0.2, stratify=dataset.merged_or_not, random_state=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Create and train the classifier\n",
    "We will now train a random forest classifier on the training set, as well as a lime tabular explainer that will be able to provide prediction's explanations based on the random forest's output. This explainer will be also assessed for trustworthiness as part of the explainability-related metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TRAIN A RANDOM FOREST CLASSIFIER\n",
    "rf_classifier = RandomForestClassifier()\n",
    "rf_classifier.fit(X_train, Y_train)\n",
    "\n",
    "# TRAIN A LIME TABULAR EXPLAINER\n",
    "lime_explainer = LimeTabularExplainer(X_train.values, feature_names=X_train.columns.values, class_names=[1,0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Store required assets\n",
    "The explainability-related metric *FaithfulnessLIMESKL* requires an additional properties, a LIME tabular explainer. Therefore, we will store such asset into the directory we have set in the configuration file. The \"explainer_path\" property corresponds to the previously fitted lime tabular explainer. We store it using dill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE EXPLAINER TO THE DEMO DIRECTORY (OPTIONAL, ONLY NECESSARY IF THEY ARE NOT ALREADY PRESENT)\n",
    "with open(demo_path + 'lime_explainer', 'wb') as explainer_file:\n",
    "    dump(lime_explainer, explainer_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Compute the trustworthiness\n",
    "Once trained, we will assess its trustworthiness with the TrustML package. For this, we instantiate a TrustworthinessComputationFacane, we call the loadTrustworthinessIndicator method with the path to the configuration file we specified, and lastly we call the computeTrustworthinessScore function, passing the trained model and the test dataset (features and target) to evaluate the model's trustworthiness in such dataset. This function stores the trust assessment as a JSON-formatted string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing p-percentage vector...\n",
      "Computing equal opportunity metric...\n",
      "Computing faithfulness metric with LIME...\n",
      "Case 212/862\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m trustpr \u001b[39m=\u001b[39m TrustworthinessComputationFacane()\n\u001b[0;32m      3\u001b[0m trustpr\u001b[39m.\u001b[39mloadTrustworthinessIndicator(configPath\u001b[39m=\u001b[39mpath_configuration)\n\u001b[1;32m----> 4\u001b[0m trustpr\u001b[39m.\u001b[39;49mcomputeTrustworthinessScore(trainedModel\u001b[39m=\u001b[39;49mrf_classifier, dataX\u001b[39m=\u001b[39;49mX_test, dataY\u001b[39m=\u001b[39;49mY_test)\n",
      "File \u001b[1;32mD:\\DOGO4ML-PYTHON\\src\\trustML\\TrustworthinessComputationFacane\\TrustworthinessComputationFacane.py:30\u001b[0m, in \u001b[0;36mTrustworthinessComputationFacane.computeTrustworthinessScore\u001b[1;34m(self, trainedModel, dataX, dataY)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcomputeTrustworthinessScore\u001b[39m(\u001b[39mself\u001b[39m, trainedModel, dataX, dataY):\n\u001b[0;32m     20\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Performs the metrics' assessments, followed by the trust assessment\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39m    based on such metrics and the specified assessment method and its parameters.\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[39m    Leverages this process to the TWI class.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39m        dataY (pandas dataset): target values of the dataset to evaluate\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mTWI\u001b[39m.\u001b[39;49massess(trainedModel, dataX, dataY)\n",
      "File \u001b[1;32mD:\\DOGO4ML-PYTHON\\src\\trustML\\TrustworthinessComputation\\TWI.py:28\u001b[0m, in \u001b[0;36mTWI.assess\u001b[1;34m(self, trainedModel, dataX, dataY)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataY \u001b[39m=\u001b[39m dataY\n\u001b[0;32m     27\u001b[0m \u001b[39mfor\u001b[39;00m metric \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetrics:\n\u001b[1;32m---> 28\u001b[0m     metric\u001b[39m.\u001b[39;49massess(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainedModel, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataX, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataY)\n\u001b[0;32m     30\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mTWS \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39massessmentMethod\u001b[39m.\u001b[39massess()\n",
      "File \u001b[1;32mD:\\DOGO4ML-PYTHON\\src\\trustML\\TrustworthinessComputation\\metrics\\FaithfulLIMESKL.py:49\u001b[0m, in \u001b[0;36mFaithfulnessLIMESKL.assess\u001b[1;34m(self, trainedModel, dataX, dataY)\u001b[0m\n\u001b[0;32m     46\u001b[0m         coefs[v[\u001b[39m0\u001b[39m]] \u001b[39m=\u001b[39m v[\u001b[39m1\u001b[39m]\n\u001b[0;32m     47\u001b[0m     base \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])\n\u001b[1;32m---> 49\u001b[0m     faithfulness_vector[i] \u001b[39m=\u001b[39m faithfulness_metric(trainedModel, dataX\u001b[39m.\u001b[39;49mvalues[i], coefs, base)\n\u001b[0;32m     50\u001b[0m scaler \u001b[39m=\u001b[39m MinMaxScaler()\n\u001b[0;32m     51\u001b[0m faithfulness_vector_scaled \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39mfit_transform(faithfulness_vector\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m)) \u001b[39m# COMPUTE FROM -1 TO 1, WE SCALED IT TO 0-1 WITH MINMAX\u001b[39;00m\n",
      "File \u001b[1;32md:\\DOGO4ML-PYTHON\\env4ml\\lib\\site-packages\\aix360\\metrics\\local_metrics.py:37\u001b[0m, in \u001b[0;36mfaithfulness_metric\u001b[1;34m(model, x, coefs, base)\u001b[0m\n\u001b[0;32m     35\u001b[0m     x_copy \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m     36\u001b[0m     x_copy[ind] \u001b[39m=\u001b[39m base[ind]\n\u001b[1;32m---> 37\u001b[0m     x_copy_pr \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict_proba(x_copy\u001b[39m.\u001b[39;49mreshape(\u001b[39m1\u001b[39;49m,\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[0;32m     38\u001b[0m     pred_probs[ind] \u001b[39m=\u001b[39m x_copy_pr[\u001b[39m0\u001b[39m][pred_class]\n\u001b[0;32m     40\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39mcorrcoef(coefs, pred_probs)[\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m]\n",
      "File \u001b[1;32md:\\DOGO4ML-PYTHON\\env4ml\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:683\u001b[0m, in \u001b[0;36mForestClassifier.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    680\u001b[0m all_proba \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39mzeros((X\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], j), dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat64)\n\u001b[0;32m    681\u001b[0m              \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m np\u001b[39m.\u001b[39matleast_1d(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_)]\n\u001b[0;32m    682\u001b[0m lock \u001b[39m=\u001b[39m threading\u001b[39m.\u001b[39mLock()\n\u001b[1;32m--> 683\u001b[0m Parallel(n_jobs\u001b[39m=\u001b[39;49mn_jobs, verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    684\u001b[0m          \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_joblib_parallel_args(require\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msharedmem\u001b[39;49m\u001b[39m\"\u001b[39;49m))(\n\u001b[0;32m    685\u001b[0m     delayed(_accumulate_prediction)(e\u001b[39m.\u001b[39;49mpredict_proba, X, all_proba,\n\u001b[0;32m    686\u001b[0m                                     lock)\n\u001b[0;32m    687\u001b[0m     \u001b[39mfor\u001b[39;49;00m e \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mestimators_)\n\u001b[0;32m    689\u001b[0m \u001b[39mfor\u001b[39;00m proba \u001b[39min\u001b[39;00m all_proba:\n\u001b[0;32m    690\u001b[0m     proba \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_)\n",
      "File \u001b[1;32md:\\DOGO4ML-PYTHON\\env4ml\\lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32md:\\DOGO4ML-PYTHON\\env4ml\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\DOGO4ML-PYTHON\\env4ml\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32md:\\DOGO4ML-PYTHON\\env4ml\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32md:\\DOGO4ML-PYTHON\\env4ml\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32md:\\DOGO4ML-PYTHON\\env4ml\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32md:\\DOGO4ML-PYTHON\\env4ml\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32md:\\DOGO4ML-PYTHON\\env4ml\\lib\\site-packages\\sklearn\\utils\\fixes.py:222\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    221\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig):\n\u001b[1;32m--> 222\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\DOGO4ML-PYTHON\\env4ml\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:467\u001b[0m, in \u001b[0;36m_accumulate_prediction\u001b[1;34m(predict, X, out, lock)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_accumulate_prediction\u001b[39m(predict, X, out, lock):\n\u001b[0;32m    461\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    462\u001b[0m \u001b[39m    This is a utility function for joblib's Parallel.\u001b[39;00m\n\u001b[0;32m    463\u001b[0m \n\u001b[0;32m    464\u001b[0m \u001b[39m    It can't go locally in ForestClassifier or ForestRegressor, because joblib\u001b[39;00m\n\u001b[0;32m    465\u001b[0m \u001b[39m    complains that it cannot pickle it when placed there.\u001b[39;00m\n\u001b[0;32m    466\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 467\u001b[0m     prediction \u001b[39m=\u001b[39m predict(X, check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    468\u001b[0m     \u001b[39mwith\u001b[39;00m lock:\n\u001b[0;32m    469\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32md:\\DOGO4ML-PYTHON\\env4ml\\lib\\site-packages\\sklearn\\tree\\_classes.py:931\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.predict_proba\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    929\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m    930\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_X_predict(X, check_input)\n\u001b[1;32m--> 931\u001b[0m proba \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_\u001b[39m.\u001b[39;49mpredict(X)\n\u001b[0;32m    933\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    934\u001b[0m     proba \u001b[39m=\u001b[39m proba[:, :\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TRUST STUFF\n",
    "trustpr = TrustworthinessComputationFacane()\n",
    "trustpr.loadTrustworthinessIndicator(configPath=path_configuration)\n",
    "trustpr.computeTrustworthinessScore(trainedModel=rf_classifier, dataX=X_test, dataY=Y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can print the complete trustworthiness assessment as a JSON-formatted string using the getTrustworthinessScore function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trustpr.getTrustworthinessScore())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which results in the following output:\n",
    "\n",
    "```javascript\n",
    "{\n",
    "  \"name\": \"Trust\",\n",
    "  \"weighted_score\": 0.8,\n",
    "  \"children\": [\n",
    "    {\n",
    "      \"name\": \"performance\",\n",
    "      \"weight\": 0.5,\n",
    "      \"weighted_score\": 0.39,\n",
    "      \"raw_score\": 0.78,\n",
    "      \"children\": [\n",
    "        {\n",
    "          \"name\": \"AccuracySKL\",\n",
    "          \"weight\": 0.7,\n",
    "          \"weighted_score\": 0.55,\n",
    "          \"raw_score\": 0.78\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"PrecisionSKL\",\n",
    "          \"weight\": 0.15,\n",
    "          \"weighted_score\": 0.12,\n",
    "          \"raw_score\": 0.78\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"RecallSKL\",\n",
    "          \"weight\": 0.15,\n",
    "          \"weighted_score\": 0.12,\n",
    "          \"raw_score\": 0.79\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"uncertainty\",\n",
    "      \"weight\": 0.15,\n",
    "      \"weighted_score\": 0.1,\n",
    "      \"raw_score\": 0.65,\n",
    "      \"children\": [\n",
    "        {\n",
    "          \"name\": \"InvertedBrierSKL\",\n",
    "          \"weight\": 0.5,\n",
    "          \"weighted_score\": 0.2,\n",
    "          \"raw_score\": 0.41\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"InvertedExpectedCalibrationSKL\",\n",
    "          \"weight\": 0.5,\n",
    "          \"weighted_score\": 0.45,\n",
    "          \"raw_score\": 0.9\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"fairness\",\n",
    "      \"weight\": 0.35,\n",
    "      \"weighted_score\": 0.32,\n",
    "      \"raw_score\": 0.9,\n",
    "      \"children\": [\n",
    "        {\n",
    "          \"name\": \"PPercentageSKL\",\n",
    "          \"weight\": 0.5,\n",
    "          \"weighted_score\": 0.46,\n",
    "          \"raw_score\": 0.92\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"EqualOpportunitySKL\",\n",
    "          \"weight\": 0.5,\n",
    "          \"weighted_score\": 0.44,\n",
    "          \"raw_score\": 0.88\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "Which results in a value of 0.8 of the trustworthiness indicator for the classification model that will conform the AI system.\n",
    "\n",
    "This notebook has illustrated how easy it is to use the TrustML package to evaluate the trustworthiness of a classification model intended to be used as part of an AI system. In this case, the TrustML package has been used as part of a model building pipeline, obtaining a trustworthiness assessment of 0.8 (out of 1). According to the trustability criteria of the ficticious open source community, the model would be deemed acceptable, as their acceptance threshold is indeed 0.8."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env4ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9717086da0a9af0457c47e23af3ec726ed6d43f3f34da84c0887679a92baa5a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>trust.Trust_Weighted_Average API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>trust.Trust_Weighted_Average</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import trust.Trust as Trust
import json

class Trust_Weighted_Average():
    &#34;&#34;&#34;Class that evaluates the Trust as a weighted average, by using the weights specified in the yaml configuration file.
    It requires an instance of a Trust object, which contains the trust metrics already computed and unweighted.
    &#34;&#34;&#34;
    
    def __init__(self, trust_instance : Trust, yaml_config_file):
        &#34;&#34;&#34;Initializer. Retrieves the weights from the yaml configuration file and assesses the trust with its metrics and
        aggregations, using the Trust instance passed as parameter.

        Args:
            trust_instance (Trust): [Instanced and initialized Trust object with unweighted metrics computed]
            yaml_config_file ([yaml module object]): [yaml dict containing the user parameters]

        Raises:
            ValueError: [When any set of related weights do not add 1]
        &#34;&#34;&#34;
        self.trust_instance = trust_instance
        self.trust_instance.feedback = self.trust_instance.get_yaml_feedback(yaml_config_file)
        
        self.weight_feedback = yaml_config_file[&#39;feedback&#39;][&#39;weight&#39;]
        self.weight_performance = yaml_config_file[&#39;w.avg_parameters&#39;][&#39;performance&#39;][&#39;weight&#39;]
        self.weight_fairness = yaml_config_file[&#39;w.avg_parameters&#39;][&#39;fairness&#39;][&#39;weight&#39;]
        self.weight_robustness = yaml_config_file[&#39;w.avg_parameters&#39;][&#39;robustness&#39;][&#39;weight&#39;]
        self.weight_explainability = yaml_config_file[&#39;w.avg_parameters&#39;][&#39;explainability&#39;][&#39;weight&#39;]
        self.weight_uncertainty = yaml_config_file[&#39;w.avg_parameters&#39;][&#39;uncertainty&#39;][&#39;weight&#39;]

        if round(self.weight_performance + self.weight_fairness + self.weight_robustness + self.weight_explainability
         + self.weight_uncertainty + self.weight_feedback, 2) != 1:
            raise ValueError(&#39;Factor weights do not add 1 (&#39; + 
            repr(self.weight_performance + self.weight_fairness + self.weight_robustness + self.weight_explainability 
            + self.weight_uncertainty + self.weight_feedback) + &#39;). Revise the configuration file.&#39;)

        # FEEDBACK #
        self.score_feedback = self.compute_score_feedback()
        # PERFORMANCE #
        self.weight_perf_accuracy, self.weight_perf_precision, self.weight_perf_recall, self.weight_perf_f1 = self.load_weights_performance(yaml_config_file)
        self.score_perf_accuracy, self.score_perf_precision, self.score_perf_recall, self.score_perf_f1, self.score_performance = self.compute_score_performance()
        # FAIRNESS #
        self.weight_fair_p_percentage, self.weight_fair_equal_opportunity_score = self.load_weights_fairness(yaml_config_file)
        self.score_fair_p_percentage, self.score_fair_equal_opportunity, self.score_fairness = self.compute_score_fairness()
        # ROBUSTNESS #
        self.weight_rob_average_bound, self.weight_rob_verified_inv_error = self.load_weights_robustness(yaml_config_file)
        self.score_rob_average_bound, self.score_rob_verified_inv_error, self.score_robustness = self.compute_score_robustness()
        # EXPLAINABILITY #
        self.score_explainability = self.weight_expl_average_monotonicity = self.weight_expl_average_faithfulness = self.weight_expl_E_accuracy_score = None
        self.score_expl_average_monotonicity = self.score_expl_average_faithfulness = self.score_expl_E_accuracy_score = None
        self.load_weights_score_explainability(yaml_config_file, self.trust_instance.explainability_method)
        # UNCERTAINTY #
        self.weight_unc_brier_inv_score, self.weight_unc_expected_cal_inv_error = self.load_weights_uncertainty(yaml_config_file)
        self.score_unc_brier_inv_score, self.score_unc_expected_cal_inv_error, self.score_uncertainty = self.compute_score_uncertainty()
        # TRUST #
        self.score_trust = self.compute_trust()
    
    def compute_trust(self):
        &#34;&#34;&#34;Adds the Trust weighted factors and returns the resulting assessed Trust&#34;&#34;&#34;
        return self.score_performance+self.score_fairness+self.score_robustness+self.score_explainability+self.score_uncertainty+self.score_feedback
        
    def compute_score_feedback(self):
        &#34;&#34;&#34;Computes the weighted feedback&#34;&#34;&#34;
        return self.trust_instance.feedback*self.weight_feedback
        
    def load_weights_performance(self, yaml_config_file):
        &#34;&#34;&#34;Retrieves the weights for the performance metrics&#34;&#34;&#34;
        weight_perf_accuracy = yaml_config_file[&#39;w.avg_parameters&#39;][&#39;performance&#39;][&#39;perf_metrics&#39;][&#39;accuracy_weight&#39;]
        weight_perf_precision = yaml_config_file[&#39;w.avg_parameters&#39;][&#39;performance&#39;][&#39;perf_metrics&#39;][&#39;precision_weight&#39;]
        weight_perf_recall = yaml_config_file[&#39;w.avg_parameters&#39;][&#39;performance&#39;][&#39;perf_metrics&#39;][&#39;recall_weight&#39;]
        weight_perf_f1 = yaml_config_file[&#39;w.avg_parameters&#39;][&#39;performance&#39;][&#39;perf_metrics&#39;][&#39;f1_weight&#39;]

        if weight_perf_accuracy + weight_perf_precision + weight_perf_recall + weight_perf_f1 != 1:
            raise ValueError(&#39;Performance metrics\&#39; weights do not add 1. Revise the configuration file&#39;)
        else:
            return weight_perf_accuracy, weight_perf_precision, weight_perf_recall, weight_perf_f1

    def compute_score_performance(self):
        &#34;&#34;&#34;Computes the weighted performance&#34;&#34;&#34;
        score_perf_accuracy = self.trust_instance.perf_accuracy * self.weight_perf_accuracy
        score_perf_precision = self.trust_instance.perf_precision * self.weight_perf_precision
        score_perf_recall = self.trust_instance.perf_recall * self.weight_perf_recall
        score_perf_f1 = self.trust_instance.perf_f1 * self.weight_perf_f1

        score_performance = (score_perf_accuracy+score_perf_precision+score_perf_recall+score_perf_f1)*self.weight_performance

        return score_perf_accuracy, score_perf_precision, score_perf_recall, score_perf_f1, score_performance
        
    def load_weights_fairness(self, yaml_config_file):
        &#34;&#34;&#34;Retrieves the weights for the fairness metrics&#34;&#34;&#34;
        weight_fair_p_percentage = yaml_config_file[&#39;w.avg_parameters&#39;][&#39;fairness&#39;][&#39;fair_metrics&#39;][&#39;p-percentage_weight&#39;]
        weight_fair_equal_opportunity_score = yaml_config_file[&#39;w.avg_parameters&#39;][&#39;fairness&#39;][&#39;fair_metrics&#39;][&#39;equal_opportunity_weight&#39;]

        if weight_fair_p_percentage + weight_fair_equal_opportunity_score != 1:
            raise ValueError(&#39;Fairness metrics\&#39; weights do not add 1. Revise the configuration file&#39;)
        else:
            return weight_fair_p_percentage, weight_fair_equal_opportunity_score

    def compute_score_fairness(self):
        &#34;&#34;&#34;Computes the weighted fairness&#34;&#34;&#34;
        score_fair_p_percentage = self.trust_instance.fair_p_percentage * self.weight_fair_p_percentage
        score_fair_equal_opportunity = self.trust_instance.fair_equal_opportunity_score * self.weight_fair_equal_opportunity_score

        score_fairness = (score_fair_p_percentage+score_fair_equal_opportunity)*self.weight_fairness

        return score_fair_p_percentage, score_fair_equal_opportunity, score_fairness

    def load_weights_robustness(self, yaml_config_file):
        &#34;&#34;&#34;Retrieves the weights for the robustness metrics&#34;&#34;&#34;
        weight_rob_average_bound = yaml_config_file[&#39;w.avg_parameters&#39;][&#39;robustness&#39;][&#39;rob_metrics&#39;][&#39;average_bound_weight&#39;]
        weight_rob_verified_inv_error = yaml_config_file[&#39;w.avg_parameters&#39;][&#39;robustness&#39;][&#39;rob_metrics&#39;][&#39;verified_error_inv_weight&#39;]

        if weight_rob_average_bound + weight_rob_verified_inv_error != 1:
            raise ValueError(&#39;Robustness metrics\&#39; weights do not add 1. Revise the configuration file&#39;)
        else:
            return weight_rob_average_bound, weight_rob_verified_inv_error

    def compute_score_robustness(self):
        &#34;&#34;&#34;Computes the weighted robustness&#34;&#34;&#34;
        score_rob_average_bound = self.trust_instance.rob_average_bound * self.weight_rob_average_bound
        score_rob_verified_inv_error = self.trust_instance.rob_verified_inv_error * self.weight_rob_verified_inv_error

        score_robustness = (score_rob_average_bound+score_rob_verified_inv_error)*self.weight_robustness

        return score_rob_average_bound, score_rob_verified_inv_error, score_robustness
    
    def load_weights_score_explainability_LIME(self, yaml_config_file):
        &#34;&#34;&#34;Retrieves the weights for the explainability metrics when using a LIME explainer&#34;&#34;&#34;
        self.weight_expl_average_monotonicity = yaml_config_file[&#39;w.avg_parameters&#39;][&#39;explainability&#39;][&#39;expl_metrics_LIME&#39;][&#39;average_monotonicity_weight&#39;]
        self.weight_expl_average_faithfulness = yaml_config_file[&#39;w.avg_parameters&#39;][&#39;explainability&#39;][&#39;expl_metrics_LIME&#39;][&#39;average_faithfulness_weight&#39;] 

        if self.weight_expl_average_monotonicity + self.weight_expl_average_faithfulness != 1:
            raise ValueError(&#39;Explainability metrics\&#39; weights (LIME) do not add 1. Revise the configuration file&#39;)
        else:            
            self.score_expl_average_monotonicity = self.trust_instance.expl_average_monotonicity * self.weight_expl_average_monotonicity
            self.score_expl_average_faithfulness = self.trust_instance.expl_average_faithfulness * self.weight_expl_average_faithfulness

            self.score_explainability = (self.score_expl_average_monotonicity+self.score_expl_average_faithfulness)*self.weight_explainability

    def load_weights_score_explainability_TED(self, yaml_config_file):
        &#34;&#34;&#34;Retrieves the weights for the explainability metrics when using a TED-enhanced classifier and explanations&#34;&#34;&#34;
        self.weight_expl_E_accuracy_score = yaml_config_file[&#39;w.avg_parameters&#39;][&#39;explainability&#39;][&#39;expl_metrics_TED&#39;][&#39;E_score_weight&#39;]

        if self.weight_expl_E_accuracy_score != 1:
            raise ValueError(&#39;Explainability metrics\&#39; weights (TED) do not add 1. Revise the configuration file&#39;)
        else:
            self.score_expl_E_accuracy_score = self.trust_instance.expl_E_accuracy*self.weight_expl_E_accuracy_score

            self.score_explainability = self.score_expl_E_accuracy_score*self.weight_explainability    

    def load_weights_score_explainability(self, yaml_config_file, explainability_method):
        &#34;&#34;&#34;Retrieves the weights for the explainability metrics&#34;&#34;&#34;
        if (explainability_method is Trust.Explainability_method.LIME):
            self.load_weights_score_explainability_LIME(yaml_config_file)
        else:
            self.load_weights_score_explainability_TED(yaml_config_file) 

    def load_weights_uncertainty(self, yaml_config_file):
        &#34;&#34;&#34;Retrieves the weights for the uncertainty metrics&#34;&#34;&#34;
        weight_unc_brier_inv_score = yaml_config_file[&#39;w.avg_parameters&#39;][&#39;uncertainty&#39;][&#39;uncert_metrics&#39;][&#39;brier_inv_score_weight&#39;]
        weight_unc_expected_cal_inv_error = yaml_config_file[&#39;w.avg_parameters&#39;][&#39;uncertainty&#39;][&#39;uncert_metrics&#39;][&#39;expected_cal_inv_error_weight&#39;] 

        if weight_unc_brier_inv_score + weight_unc_expected_cal_inv_error != 1:
            raise ValueError(&#39;Uncertainty metrics\&#39; weights do not add 1. Revise the configuration file&#39;)
        else:
            return weight_unc_brier_inv_score, weight_unc_expected_cal_inv_error

    def compute_score_uncertainty(self):
        &#34;&#34;&#34;Computes the weighted uncertainty&#34;&#34;&#34;
        score_unc_brier_inv_score = self.trust_instance.unc_brier_inv_score * self.weight_unc_brier_inv_score
        score_unc_expected_cal_inv_error = self.trust_instance.unc_expected_cal_inv_error * self.weight_unc_expected_cal_inv_error

        score_uncertainty = (score_unc_brier_inv_score+score_unc_expected_cal_inv_error)*self.weight_uncertainty

        return score_unc_brier_inv_score, score_unc_expected_cal_inv_error, score_uncertainty

    def __str__(self):
        &#34;&#34;&#34;Returns a string with the assessed Trust along with its assessed factors&#34;&#34;&#34;
        return &#34;TRUST EVALUATED AS A WEIGHTED AVERAGE: {:.2f}&#34;.format(self.score_trust) + &#34;\nWeighted Performance: {:.2f}&#34;.format(self.score_performance) + \
            &#34;\nWeighted Fairness: {:.2f}&#34;.format(self.score_fairness) + &#34;\nWeighted Uncertainty: {:.2f}&#34;.format(self.score_uncertainty) \
                + &#34;\nWeighted Explainability: {:.2f}&#34;.format(self.score_explainability) + &#34;\nWeighted Robustness: {:.2f}&#34;.format(self.score_robustness) \
                    + &#34;\nWeighted Feedback: {:.2f}&#34;.format(self.score_feedback)

    def get_Trust_WA_as_JSON(self):
        &#34;&#34;&#34;Returns the weighted and assessed trust along with its components and used weights as a JSON&#34;&#34;&#34;
        # Create Dictionary
        trust_assessment = {
            &#34;ABOUT&#34;: &#34;TRUST AND ITS COMPONENTS ASSESSED AS A WEIGHTED AVERAGE&#34;,
            &#34;TRUST&#34;: self.score_trust,
            &#34;feedback&#34;:
                {
                    &#34;weight&#34;: self.weight_feedback,
                    &#34;score&#34;: self.score_feedback
                },
            &#34;performance&#34;:
                {
                    &#34;weight&#34;: self.weight_performance,
                    &#34;score&#34;: self.score_performance,
                    &#34;accuracy&#34;:
                        {
                            &#34;weight&#34;: self.weight_perf_accuracy,
                            &#34;score&#34;: self.score_perf_accuracy
                        },                        
                    &#34;precision&#34;:
                        {
                            &#34;weight&#34;: self.weight_perf_precision,
                            &#34;score&#34;:  self.score_perf_precision
                        },
                    &#34;recall&#34;:
                        {
                            &#34;weight&#34;: self.weight_perf_recall,
                            &#34;score&#34;: self.score_perf_recall
                        }
                },
            &#34;fairness&#34;:
                {
                    &#34;weight&#34;: self.weight_fairness,
                    &#34;score&#34;: self.score_fairness,
                    &#34;p_percentage&#34;:
                        {
                            &#34;weight&#34;: self.weight_fair_p_percentage,
                            &#34;score&#34;: self.score_fair_p_percentage
                        },
                    &#34;equal_opportunity&#34;:
                        {
                            &#34;weight&#34;: self.weight_fair_equal_opportunity_score,
                            &#34;score&#34;: self.score_fair_equal_opportunity
                        }
                },
            &#34;robustness&#34;:
                {
                    &#34;weight&#34;: self.weight_robustness,
                    &#34;score&#34;: self.score_robustness,
                    &#34;average_bound&#34;:
                        {
                            &#34;weight&#34;: self.weight_rob_average_bound,
                            &#34;score&#34;: self.score_rob_average_bound
                        },
                    &#34;verified_error_inv&#34;:
                        {
                            &#34;weight&#34;: self.weight_rob_verified_inv_error,
                            &#34;score&#34;: self.score_rob_verified_inv_error
                        }
                },
            &#34;explainability&#34;:
                {
                    &#34;weight&#34;: self.weight_explainability,
                    &#34;score&#34;: self.score_explainability,
                    &#34;average_monotonicity_LIME&#34;:
                        {
                            &#34;weight&#34;: self.weight_expl_average_monotonicity,
                            &#34;score&#34;: self.score_expl_average_monotonicity
                        },
                    &#34;average_faithfulness_LIME&#34;:
                        {
                            &#34;weight&#34;: self.weight_expl_average_faithfulness,
                            &#34;score&#34;: self.score_expl_average_faithfulness
                        },
                    &#34;E_score_TED&#34;:
                        {
                            &#34;weight&#34;: self.weight_expl_E_accuracy_score,
                            &#34;score&#34;: self.score_expl_E_accuracy_score
                        }                    
                },
            &#34;uncertainty&#34;:
                {
                    &#34;weight&#34;: self.weight_uncertainty,
                    &#34;score&#34;: self.score_uncertainty,
                    &#34;brier_score&#34;:
                        {
                            &#34;weight&#34;: self.weight_unc_brier_inv_score,
                            &#34;score&#34;: self.score_unc_brier_inv_score
                        },
                    &#34;expected_calibration_error_inv&#34;:
                        {
                            &#34;weight&#34;: self.weight_unc_expected_cal_inv_error,
                            &#34;score&#34;: self.score_unc_expected_cal_inv_error
                        }
                    
                }
        }
        return json.dumps(trust_assessment, indent=4)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="trust.Trust_Weighted_Average.Trust_Weighted_Average"><code class="flex name class">
<span>class <span class="ident">Trust_Weighted_Average</span></span>
<span>(</span><span>trust_instance: <module '<a title="trust.Trust" href="Trust.html">trust.Trust</a>' from '/mnt/d/Dropbox/DOGO4ML-PYTHON/<a title="trust" href="index.html">trust</a>/Trust.py'>, yaml_config_file)</span>
</code></dt>
<dd>
<div class="desc"><p>Class that evaluates the Trust as a weighted average, by using the weights specified in the yaml configuration file.
It requires an instance of a Trust object, which contains the trust metrics already computed and unweighted.</p>
<p>Initializer. Retrieves the weights from the yaml configuration file and assesses the trust with its metrics and
aggregations, using the Trust instance passed as parameter.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>trust_instance</code></strong> :&ensp;<code>Trust</code></dt>
<dd>[Instanced and initialized Trust object with unweighted metrics computed]</dd>
<dt><strong><code>yaml_config_file</code></strong> :&ensp;<code>[yaml module object]</code></dt>
<dd>[yaml dict containing the user parameters]</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>[When any set of related weights do not add 1]</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Trust_Weighted_Average():
    &#34;&#34;&#34;Class that evaluates the Trust as a weighted average, by using the weights specified in the yaml configuration file.
    It requires an instance of a Trust object, which contains the trust metrics already computed and unweighted.
    &#34;&#34;&#34;
    
    def __init__(self, trust_instance : Trust, yaml_config_file):
        &#34;&#34;&#34;Initializer. Retrieves the weights from the yaml configuration file and assesses the trust with its metrics and
        aggregations, using the Trust instance passed as parameter.

        Args:
            trust_instance (Trust): [Instanced and initialized Trust object with unweighted metrics computed]
            yaml_config_file ([yaml module object]): [yaml dict containing the user parameters]

        Raises:
            ValueError: [When any set of related weights do not add 1]
        &#34;&#34;&#34;
        self.trust_instance = trust_instance
        self.trust_instance.feedback = self.trust_instance.get_yaml_feedback(yaml_config_file)
        
        self.weight_feedback = yaml_config_file[&#39;feedback&#39;][&#39;weight&#39;]
        self.weight_performance = yaml_config_file[&#39;w.avg_parameters&#39;][&#39;performance&#39;][&#39;weight&#39;]
        self.weight_fairness = yaml_config_file[&#39;w.avg_parameters&#39;][&#39;fairness&#39;][&#39;weight&#39;]
        self.weight_robustness = yaml_config_file[&#39;w.avg_parameters&#39;][&#39;robustness&#39;][&#39;weight&#39;]
        self.weight_explainability = yaml_config_file[&#39;w.avg_parameters&#39;][&#39;explainability&#39;][&#39;weight&#39;]
        self.weight_uncertainty = yaml_config_file[&#39;w.avg_parameters&#39;][&#39;uncertainty&#39;][&#39;weight&#39;]

        if round(self.weight_performance + self.weight_fairness + self.weight_robustness + self.weight_explainability
         + self.weight_uncertainty + self.weight_feedback, 2) != 1:
            raise ValueError(&#39;Factor weights do not add 1 (&#39; + 
            repr(self.weight_performance + self.weight_fairness + self.weight_robustness + self.weight_explainability 
            + self.weight_uncertainty + self.weight_feedback) + &#39;). Revise the configuration file.&#39;)

        # FEEDBACK #
        self.score_feedback = self.compute_score_feedback()
        # PERFORMANCE #
        self.weight_perf_accuracy, self.weight_perf_precision, self.weight_perf_recall, self.weight_perf_f1 = self.load_weights_performance(yaml_config_file)
        self.score_perf_accuracy, self.score_perf_precision, self.score_perf_recall, self.score_perf_f1, self.score_performance = self.compute_score_performance()
        # FAIRNESS #
        self.weight_fair_p_percentage, self.weight_fair_equal_opportunity_score = self.load_weights_fairness(yaml_config_file)
        self.score_fair_p_percentage, self.score_fair_equal_opportunity, self.score_fairness = self.compute_score_fairness()
        # ROBUSTNESS #
        self.weight_rob_average_bound, self.weight_rob_verified_inv_error = self.load_weights_robustness(yaml_config_file)
        self.score_rob_average_bound, self.score_rob_verified_inv_error, self.score_robustness = self.compute_score_robustness()
        # EXPLAINABILITY #
        self.score_explainability = self.weight_expl_average_monotonicity = self.weight_expl_average_faithfulness = self.weight_expl_E_accuracy_score = None
        self.score_expl_average_monotonicity = self.score_expl_average_faithfulness = self.score_expl_E_accuracy_score = None
        self.load_weights_score_explainability(yaml_config_file, self.trust_instance.explainability_method)
        # UNCERTAINTY #
        self.weight_unc_brier_inv_score, self.weight_unc_expected_cal_inv_error = self.load_weights_uncertainty(yaml_config_file)
        self.score_unc_brier_inv_score, self.score_unc_expected_cal_inv_error, self.score_uncertainty = self.compute_score_uncertainty()
        # TRUST #
        self.score_trust = self.compute_trust()
    
    def compute_trust(self):
        &#34;&#34;&#34;Adds the Trust weighted factors and returns the resulting assessed Trust&#34;&#34;&#34;
        return self.score_performance+self.score_fairness+self.score_robustness+self.score_explainability+self.score_uncertainty+self.score_feedback
        
    def compute_score_feedback(self):
        &#34;&#34;&#34;Computes the weighted feedback&#34;&#34;&#34;
        return self.trust_instance.feedback*self.weight_feedback
        
    def load_weights_performance(self, yaml_config_file):
        &#34;&#34;&#34;Retrieves the weights for the performance metrics&#34;&#34;&#34;
        weight_perf_accuracy = yaml_config_file[&#39;w.avg_parameters&#39;][&#39;performance&#39;][&#39;perf_metrics&#39;][&#39;accuracy_weight&#39;]
        weight_perf_precision = yaml_config_file[&#39;w.avg_parameters&#39;][&#39;performance&#39;][&#39;perf_metrics&#39;][&#39;precision_weight&#39;]
        weight_perf_recall = yaml_config_file[&#39;w.avg_parameters&#39;][&#39;performance&#39;][&#39;perf_metrics&#39;][&#39;recall_weight&#39;]
        weight_perf_f1 = yaml_config_file[&#39;w.avg_parameters&#39;][&#39;performance&#39;][&#39;perf_metrics&#39;][&#39;f1_weight&#39;]

        if weight_perf_accuracy + weight_perf_precision + weight_perf_recall + weight_perf_f1 != 1:
            raise ValueError(&#39;Performance metrics\&#39; weights do not add 1. Revise the configuration file&#39;)
        else:
            return weight_perf_accuracy, weight_perf_precision, weight_perf_recall, weight_perf_f1

    def compute_score_performance(self):
        &#34;&#34;&#34;Computes the weighted performance&#34;&#34;&#34;
        score_perf_accuracy = self.trust_instance.perf_accuracy * self.weight_perf_accuracy
        score_perf_precision = self.trust_instance.perf_precision * self.weight_perf_precision
        score_perf_recall = self.trust_instance.perf_recall * self.weight_perf_recall
        score_perf_f1 = self.trust_instance.perf_f1 * self.weight_perf_f1

        score_performance = (score_perf_accuracy+score_perf_precision+score_perf_recall+score_perf_f1)*self.weight_performance

        return score_perf_accuracy, score_perf_precision, score_perf_recall, score_perf_f1, score_performance
        
    def load_weights_fairness(self, yaml_config_file):
        &#34;&#34;&#34;Retrieves the weights for the fairness metrics&#34;&#34;&#34;
        weight_fair_p_percentage = yaml_config_file[&#39;w.avg_parameters&#39;][&#39;fairness&#39;][&#39;fair_metrics&#39;][&#39;p-percentage_weight&#39;]
        weight_fair_equal_opportunity_score = yaml_config_file[&#39;w.avg_parameters&#39;][&#39;fairness&#39;][&#39;fair_metrics&#39;][&#39;equal_opportunity_weight&#39;]

        if weight_fair_p_percentage + weight_fair_equal_opportunity_score != 1:
            raise ValueError(&#39;Fairness metrics\&#39; weights do not add 1. Revise the configuration file&#39;)
        else:
            return weight_fair_p_percentage, weight_fair_equal_opportunity_score

    def compute_score_fairness(self):
        &#34;&#34;&#34;Computes the weighted fairness&#34;&#34;&#34;
        score_fair_p_percentage = self.trust_instance.fair_p_percentage * self.weight_fair_p_percentage
        score_fair_equal_opportunity = self.trust_instance.fair_equal_opportunity_score * self.weight_fair_equal_opportunity_score

        score_fairness = (score_fair_p_percentage+score_fair_equal_opportunity)*self.weight_fairness

        return score_fair_p_percentage, score_fair_equal_opportunity, score_fairness

    def load_weights_robustness(self, yaml_config_file):
        &#34;&#34;&#34;Retrieves the weights for the robustness metrics&#34;&#34;&#34;
        weight_rob_average_bound = yaml_config_file[&#39;w.avg_parameters&#39;][&#39;robustness&#39;][&#39;rob_metrics&#39;][&#39;average_bound_weight&#39;]
        weight_rob_verified_inv_error = yaml_config_file[&#39;w.avg_parameters&#39;][&#39;robustness&#39;][&#39;rob_metrics&#39;][&#39;verified_error_inv_weight&#39;]

        if weight_rob_average_bound + weight_rob_verified_inv_error != 1:
            raise ValueError(&#39;Robustness metrics\&#39; weights do not add 1. Revise the configuration file&#39;)
        else:
            return weight_rob_average_bound, weight_rob_verified_inv_error

    def compute_score_robustness(self):
        &#34;&#34;&#34;Computes the weighted robustness&#34;&#34;&#34;
        score_rob_average_bound = self.trust_instance.rob_average_bound * self.weight_rob_average_bound
        score_rob_verified_inv_error = self.trust_instance.rob_verified_inv_error * self.weight_rob_verified_inv_error

        score_robustness = (score_rob_average_bound+score_rob_verified_inv_error)*self.weight_robustness

        return score_rob_average_bound, score_rob_verified_inv_error, score_robustness
    
    def load_weights_score_explainability_LIME(self, yaml_config_file):
        &#34;&#34;&#34;Retrieves the weights for the explainability metrics when using a LIME explainer&#34;&#34;&#34;
        self.weight_expl_average_monotonicity = yaml_config_file[&#39;w.avg_parameters&#39;][&#39;explainability&#39;][&#39;expl_metrics_LIME&#39;][&#39;average_monotonicity_weight&#39;]
        self.weight_expl_average_faithfulness = yaml_config_file[&#39;w.avg_parameters&#39;][&#39;explainability&#39;][&#39;expl_metrics_LIME&#39;][&#39;average_faithfulness_weight&#39;] 

        if self.weight_expl_average_monotonicity + self.weight_expl_average_faithfulness != 1:
            raise ValueError(&#39;Explainability metrics\&#39; weights (LIME) do not add 1. Revise the configuration file&#39;)
        else:            
            self.score_expl_average_monotonicity = self.trust_instance.expl_average_monotonicity * self.weight_expl_average_monotonicity
            self.score_expl_average_faithfulness = self.trust_instance.expl_average_faithfulness * self.weight_expl_average_faithfulness

            self.score_explainability = (self.score_expl_average_monotonicity+self.score_expl_average_faithfulness)*self.weight_explainability

    def load_weights_score_explainability_TED(self, yaml_config_file):
        &#34;&#34;&#34;Retrieves the weights for the explainability metrics when using a TED-enhanced classifier and explanations&#34;&#34;&#34;
        self.weight_expl_E_accuracy_score = yaml_config_file[&#39;w.avg_parameters&#39;][&#39;explainability&#39;][&#39;expl_metrics_TED&#39;][&#39;E_score_weight&#39;]

        if self.weight_expl_E_accuracy_score != 1:
            raise ValueError(&#39;Explainability metrics\&#39; weights (TED) do not add 1. Revise the configuration file&#39;)
        else:
            self.score_expl_E_accuracy_score = self.trust_instance.expl_E_accuracy*self.weight_expl_E_accuracy_score

            self.score_explainability = self.score_expl_E_accuracy_score*self.weight_explainability    

    def load_weights_score_explainability(self, yaml_config_file, explainability_method):
        &#34;&#34;&#34;Retrieves the weights for the explainability metrics&#34;&#34;&#34;
        if (explainability_method is Trust.Explainability_method.LIME):
            self.load_weights_score_explainability_LIME(yaml_config_file)
        else:
            self.load_weights_score_explainability_TED(yaml_config_file) 

    def load_weights_uncertainty(self, yaml_config_file):
        &#34;&#34;&#34;Retrieves the weights for the uncertainty metrics&#34;&#34;&#34;
        weight_unc_brier_inv_score = yaml_config_file[&#39;w.avg_parameters&#39;][&#39;uncertainty&#39;][&#39;uncert_metrics&#39;][&#39;brier_inv_score_weight&#39;]
        weight_unc_expected_cal_inv_error = yaml_config_file[&#39;w.avg_parameters&#39;][&#39;uncertainty&#39;][&#39;uncert_metrics&#39;][&#39;expected_cal_inv_error_weight&#39;] 

        if weight_unc_brier_inv_score + weight_unc_expected_cal_inv_error != 1:
            raise ValueError(&#39;Uncertainty metrics\&#39; weights do not add 1. Revise the configuration file&#39;)
        else:
            return weight_unc_brier_inv_score, weight_unc_expected_cal_inv_error

    def compute_score_uncertainty(self):
        &#34;&#34;&#34;Computes the weighted uncertainty&#34;&#34;&#34;
        score_unc_brier_inv_score = self.trust_instance.unc_brier_inv_score * self.weight_unc_brier_inv_score
        score_unc_expected_cal_inv_error = self.trust_instance.unc_expected_cal_inv_error * self.weight_unc_expected_cal_inv_error

        score_uncertainty = (score_unc_brier_inv_score+score_unc_expected_cal_inv_error)*self.weight_uncertainty

        return score_unc_brier_inv_score, score_unc_expected_cal_inv_error, score_uncertainty

    def __str__(self):
        &#34;&#34;&#34;Returns a string with the assessed Trust along with its assessed factors&#34;&#34;&#34;
        return &#34;TRUST EVALUATED AS A WEIGHTED AVERAGE: {:.2f}&#34;.format(self.score_trust) + &#34;\nWeighted Performance: {:.2f}&#34;.format(self.score_performance) + \
            &#34;\nWeighted Fairness: {:.2f}&#34;.format(self.score_fairness) + &#34;\nWeighted Uncertainty: {:.2f}&#34;.format(self.score_uncertainty) \
                + &#34;\nWeighted Explainability: {:.2f}&#34;.format(self.score_explainability) + &#34;\nWeighted Robustness: {:.2f}&#34;.format(self.score_robustness) \
                    + &#34;\nWeighted Feedback: {:.2f}&#34;.format(self.score_feedback)

    def get_Trust_WA_as_JSON(self):
        &#34;&#34;&#34;Returns the weighted and assessed trust along with its components and used weights as a JSON&#34;&#34;&#34;
        # Create Dictionary
        trust_assessment = {
            &#34;ABOUT&#34;: &#34;TRUST AND ITS COMPONENTS ASSESSED AS A WEIGHTED AVERAGE&#34;,
            &#34;TRUST&#34;: self.score_trust,
            &#34;feedback&#34;:
                {
                    &#34;weight&#34;: self.weight_feedback,
                    &#34;score&#34;: self.score_feedback
                },
            &#34;performance&#34;:
                {
                    &#34;weight&#34;: self.weight_performance,
                    &#34;score&#34;: self.score_performance,
                    &#34;accuracy&#34;:
                        {
                            &#34;weight&#34;: self.weight_perf_accuracy,
                            &#34;score&#34;: self.score_perf_accuracy
                        },                        
                    &#34;precision&#34;:
                        {
                            &#34;weight&#34;: self.weight_perf_precision,
                            &#34;score&#34;:  self.score_perf_precision
                        },
                    &#34;recall&#34;:
                        {
                            &#34;weight&#34;: self.weight_perf_recall,
                            &#34;score&#34;: self.score_perf_recall
                        }
                },
            &#34;fairness&#34;:
                {
                    &#34;weight&#34;: self.weight_fairness,
                    &#34;score&#34;: self.score_fairness,
                    &#34;p_percentage&#34;:
                        {
                            &#34;weight&#34;: self.weight_fair_p_percentage,
                            &#34;score&#34;: self.score_fair_p_percentage
                        },
                    &#34;equal_opportunity&#34;:
                        {
                            &#34;weight&#34;: self.weight_fair_equal_opportunity_score,
                            &#34;score&#34;: self.score_fair_equal_opportunity
                        }
                },
            &#34;robustness&#34;:
                {
                    &#34;weight&#34;: self.weight_robustness,
                    &#34;score&#34;: self.score_robustness,
                    &#34;average_bound&#34;:
                        {
                            &#34;weight&#34;: self.weight_rob_average_bound,
                            &#34;score&#34;: self.score_rob_average_bound
                        },
                    &#34;verified_error_inv&#34;:
                        {
                            &#34;weight&#34;: self.weight_rob_verified_inv_error,
                            &#34;score&#34;: self.score_rob_verified_inv_error
                        }
                },
            &#34;explainability&#34;:
                {
                    &#34;weight&#34;: self.weight_explainability,
                    &#34;score&#34;: self.score_explainability,
                    &#34;average_monotonicity_LIME&#34;:
                        {
                            &#34;weight&#34;: self.weight_expl_average_monotonicity,
                            &#34;score&#34;: self.score_expl_average_monotonicity
                        },
                    &#34;average_faithfulness_LIME&#34;:
                        {
                            &#34;weight&#34;: self.weight_expl_average_faithfulness,
                            &#34;score&#34;: self.score_expl_average_faithfulness
                        },
                    &#34;E_score_TED&#34;:
                        {
                            &#34;weight&#34;: self.weight_expl_E_accuracy_score,
                            &#34;score&#34;: self.score_expl_E_accuracy_score
                        }                    
                },
            &#34;uncertainty&#34;:
                {
                    &#34;weight&#34;: self.weight_uncertainty,
                    &#34;score&#34;: self.score_uncertainty,
                    &#34;brier_score&#34;:
                        {
                            &#34;weight&#34;: self.weight_unc_brier_inv_score,
                            &#34;score&#34;: self.score_unc_brier_inv_score
                        },
                    &#34;expected_calibration_error_inv&#34;:
                        {
                            &#34;weight&#34;: self.weight_unc_expected_cal_inv_error,
                            &#34;score&#34;: self.score_unc_expected_cal_inv_error
                        }
                    
                }
        }
        return json.dumps(trust_assessment, indent=4)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="trust.Trust_Weighted_Average.Trust_Weighted_Average.compute_score_fairness"><code class="name flex">
<span>def <span class="ident">compute_score_fairness</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the weighted fairness</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_score_fairness(self):
    &#34;&#34;&#34;Computes the weighted fairness&#34;&#34;&#34;
    score_fair_p_percentage = self.trust_instance.fair_p_percentage * self.weight_fair_p_percentage
    score_fair_equal_opportunity = self.trust_instance.fair_equal_opportunity_score * self.weight_fair_equal_opportunity_score

    score_fairness = (score_fair_p_percentage+score_fair_equal_opportunity)*self.weight_fairness

    return score_fair_p_percentage, score_fair_equal_opportunity, score_fairness</code></pre>
</details>
</dd>
<dt id="trust.Trust_Weighted_Average.Trust_Weighted_Average.compute_score_feedback"><code class="name flex">
<span>def <span class="ident">compute_score_feedback</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the weighted feedback</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_score_feedback(self):
    &#34;&#34;&#34;Computes the weighted feedback&#34;&#34;&#34;
    return self.trust_instance.feedback*self.weight_feedback</code></pre>
</details>
</dd>
<dt id="trust.Trust_Weighted_Average.Trust_Weighted_Average.compute_score_performance"><code class="name flex">
<span>def <span class="ident">compute_score_performance</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the weighted performance</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_score_performance(self):
    &#34;&#34;&#34;Computes the weighted performance&#34;&#34;&#34;
    score_perf_accuracy = self.trust_instance.perf_accuracy * self.weight_perf_accuracy
    score_perf_precision = self.trust_instance.perf_precision * self.weight_perf_precision
    score_perf_recall = self.trust_instance.perf_recall * self.weight_perf_recall
    score_perf_f1 = self.trust_instance.perf_f1 * self.weight_perf_f1

    score_performance = (score_perf_accuracy+score_perf_precision+score_perf_recall+score_perf_f1)*self.weight_performance

    return score_perf_accuracy, score_perf_precision, score_perf_recall, score_perf_f1, score_performance</code></pre>
</details>
</dd>
<dt id="trust.Trust_Weighted_Average.Trust_Weighted_Average.compute_score_robustness"><code class="name flex">
<span>def <span class="ident">compute_score_robustness</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the weighted robustness</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_score_robustness(self):
    &#34;&#34;&#34;Computes the weighted robustness&#34;&#34;&#34;
    score_rob_average_bound = self.trust_instance.rob_average_bound * self.weight_rob_average_bound
    score_rob_verified_inv_error = self.trust_instance.rob_verified_inv_error * self.weight_rob_verified_inv_error

    score_robustness = (score_rob_average_bound+score_rob_verified_inv_error)*self.weight_robustness

    return score_rob_average_bound, score_rob_verified_inv_error, score_robustness</code></pre>
</details>
</dd>
<dt id="trust.Trust_Weighted_Average.Trust_Weighted_Average.compute_score_uncertainty"><code class="name flex">
<span>def <span class="ident">compute_score_uncertainty</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the weighted uncertainty</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_score_uncertainty(self):
    &#34;&#34;&#34;Computes the weighted uncertainty&#34;&#34;&#34;
    score_unc_brier_inv_score = self.trust_instance.unc_brier_inv_score * self.weight_unc_brier_inv_score
    score_unc_expected_cal_inv_error = self.trust_instance.unc_expected_cal_inv_error * self.weight_unc_expected_cal_inv_error

    score_uncertainty = (score_unc_brier_inv_score+score_unc_expected_cal_inv_error)*self.weight_uncertainty

    return score_unc_brier_inv_score, score_unc_expected_cal_inv_error, score_uncertainty</code></pre>
</details>
</dd>
<dt id="trust.Trust_Weighted_Average.Trust_Weighted_Average.compute_trust"><code class="name flex">
<span>def <span class="ident">compute_trust</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Adds the Trust weighted factors and returns the resulting assessed Trust</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_trust(self):
    &#34;&#34;&#34;Adds the Trust weighted factors and returns the resulting assessed Trust&#34;&#34;&#34;
    return self.score_performance+self.score_fairness+self.score_robustness+self.score_explainability+self.score_uncertainty+self.score_feedback</code></pre>
</details>
</dd>
<dt id="trust.Trust_Weighted_Average.Trust_Weighted_Average.get_Trust_WA_as_JSON"><code class="name flex">
<span>def <span class="ident">get_Trust_WA_as_JSON</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the weighted and assessed trust along with its components and used weights as a JSON</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_Trust_WA_as_JSON(self):
    &#34;&#34;&#34;Returns the weighted and assessed trust along with its components and used weights as a JSON&#34;&#34;&#34;
    # Create Dictionary
    trust_assessment = {
        &#34;ABOUT&#34;: &#34;TRUST AND ITS COMPONENTS ASSESSED AS A WEIGHTED AVERAGE&#34;,
        &#34;TRUST&#34;: self.score_trust,
        &#34;feedback&#34;:
            {
                &#34;weight&#34;: self.weight_feedback,
                &#34;score&#34;: self.score_feedback
            },
        &#34;performance&#34;:
            {
                &#34;weight&#34;: self.weight_performance,
                &#34;score&#34;: self.score_performance,
                &#34;accuracy&#34;:
                    {
                        &#34;weight&#34;: self.weight_perf_accuracy,
                        &#34;score&#34;: self.score_perf_accuracy
                    },                        
                &#34;precision&#34;:
                    {
                        &#34;weight&#34;: self.weight_perf_precision,
                        &#34;score&#34;:  self.score_perf_precision
                    },
                &#34;recall&#34;:
                    {
                        &#34;weight&#34;: self.weight_perf_recall,
                        &#34;score&#34;: self.score_perf_recall
                    }
            },
        &#34;fairness&#34;:
            {
                &#34;weight&#34;: self.weight_fairness,
                &#34;score&#34;: self.score_fairness,
                &#34;p_percentage&#34;:
                    {
                        &#34;weight&#34;: self.weight_fair_p_percentage,
                        &#34;score&#34;: self.score_fair_p_percentage
                    },
                &#34;equal_opportunity&#34;:
                    {
                        &#34;weight&#34;: self.weight_fair_equal_opportunity_score,
                        &#34;score&#34;: self.score_fair_equal_opportunity
                    }
            },
        &#34;robustness&#34;:
            {
                &#34;weight&#34;: self.weight_robustness,
                &#34;score&#34;: self.score_robustness,
                &#34;average_bound&#34;:
                    {
                        &#34;weight&#34;: self.weight_rob_average_bound,
                        &#34;score&#34;: self.score_rob_average_bound
                    },
                &#34;verified_error_inv&#34;:
                    {
                        &#34;weight&#34;: self.weight_rob_verified_inv_error,
                        &#34;score&#34;: self.score_rob_verified_inv_error
                    }
            },
        &#34;explainability&#34;:
            {
                &#34;weight&#34;: self.weight_explainability,
                &#34;score&#34;: self.score_explainability,
                &#34;average_monotonicity_LIME&#34;:
                    {
                        &#34;weight&#34;: self.weight_expl_average_monotonicity,
                        &#34;score&#34;: self.score_expl_average_monotonicity
                    },
                &#34;average_faithfulness_LIME&#34;:
                    {
                        &#34;weight&#34;: self.weight_expl_average_faithfulness,
                        &#34;score&#34;: self.score_expl_average_faithfulness
                    },
                &#34;E_score_TED&#34;:
                    {
                        &#34;weight&#34;: self.weight_expl_E_accuracy_score,
                        &#34;score&#34;: self.score_expl_E_accuracy_score
                    }                    
            },
        &#34;uncertainty&#34;:
            {
                &#34;weight&#34;: self.weight_uncertainty,
                &#34;score&#34;: self.score_uncertainty,
                &#34;brier_score&#34;:
                    {
                        &#34;weight&#34;: self.weight_unc_brier_inv_score,
                        &#34;score&#34;: self.score_unc_brier_inv_score
                    },
                &#34;expected_calibration_error_inv&#34;:
                    {
                        &#34;weight&#34;: self.weight_unc_expected_cal_inv_error,
                        &#34;score&#34;: self.score_unc_expected_cal_inv_error
                    }
                
            }
    }
    return json.dumps(trust_assessment, indent=4)</code></pre>
</details>
</dd>
<dt id="trust.Trust_Weighted_Average.Trust_Weighted_Average.load_weights_fairness"><code class="name flex">
<span>def <span class="ident">load_weights_fairness</span></span>(<span>self, yaml_config_file)</span>
</code></dt>
<dd>
<div class="desc"><p>Retrieves the weights for the fairness metrics</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_weights_fairness(self, yaml_config_file):
    &#34;&#34;&#34;Retrieves the weights for the fairness metrics&#34;&#34;&#34;
    weight_fair_p_percentage = yaml_config_file[&#39;w.avg_parameters&#39;][&#39;fairness&#39;][&#39;fair_metrics&#39;][&#39;p-percentage_weight&#39;]
    weight_fair_equal_opportunity_score = yaml_config_file[&#39;w.avg_parameters&#39;][&#39;fairness&#39;][&#39;fair_metrics&#39;][&#39;equal_opportunity_weight&#39;]

    if weight_fair_p_percentage + weight_fair_equal_opportunity_score != 1:
        raise ValueError(&#39;Fairness metrics\&#39; weights do not add 1. Revise the configuration file&#39;)
    else:
        return weight_fair_p_percentage, weight_fair_equal_opportunity_score</code></pre>
</details>
</dd>
<dt id="trust.Trust_Weighted_Average.Trust_Weighted_Average.load_weights_performance"><code class="name flex">
<span>def <span class="ident">load_weights_performance</span></span>(<span>self, yaml_config_file)</span>
</code></dt>
<dd>
<div class="desc"><p>Retrieves the weights for the performance metrics</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_weights_performance(self, yaml_config_file):
    &#34;&#34;&#34;Retrieves the weights for the performance metrics&#34;&#34;&#34;
    weight_perf_accuracy = yaml_config_file[&#39;w.avg_parameters&#39;][&#39;performance&#39;][&#39;perf_metrics&#39;][&#39;accuracy_weight&#39;]
    weight_perf_precision = yaml_config_file[&#39;w.avg_parameters&#39;][&#39;performance&#39;][&#39;perf_metrics&#39;][&#39;precision_weight&#39;]
    weight_perf_recall = yaml_config_file[&#39;w.avg_parameters&#39;][&#39;performance&#39;][&#39;perf_metrics&#39;][&#39;recall_weight&#39;]
    weight_perf_f1 = yaml_config_file[&#39;w.avg_parameters&#39;][&#39;performance&#39;][&#39;perf_metrics&#39;][&#39;f1_weight&#39;]

    if weight_perf_accuracy + weight_perf_precision + weight_perf_recall + weight_perf_f1 != 1:
        raise ValueError(&#39;Performance metrics\&#39; weights do not add 1. Revise the configuration file&#39;)
    else:
        return weight_perf_accuracy, weight_perf_precision, weight_perf_recall, weight_perf_f1</code></pre>
</details>
</dd>
<dt id="trust.Trust_Weighted_Average.Trust_Weighted_Average.load_weights_robustness"><code class="name flex">
<span>def <span class="ident">load_weights_robustness</span></span>(<span>self, yaml_config_file)</span>
</code></dt>
<dd>
<div class="desc"><p>Retrieves the weights for the robustness metrics</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_weights_robustness(self, yaml_config_file):
    &#34;&#34;&#34;Retrieves the weights for the robustness metrics&#34;&#34;&#34;
    weight_rob_average_bound = yaml_config_file[&#39;w.avg_parameters&#39;][&#39;robustness&#39;][&#39;rob_metrics&#39;][&#39;average_bound_weight&#39;]
    weight_rob_verified_inv_error = yaml_config_file[&#39;w.avg_parameters&#39;][&#39;robustness&#39;][&#39;rob_metrics&#39;][&#39;verified_error_inv_weight&#39;]

    if weight_rob_average_bound + weight_rob_verified_inv_error != 1:
        raise ValueError(&#39;Robustness metrics\&#39; weights do not add 1. Revise the configuration file&#39;)
    else:
        return weight_rob_average_bound, weight_rob_verified_inv_error</code></pre>
</details>
</dd>
<dt id="trust.Trust_Weighted_Average.Trust_Weighted_Average.load_weights_score_explainability"><code class="name flex">
<span>def <span class="ident">load_weights_score_explainability</span></span>(<span>self, yaml_config_file, explainability_method)</span>
</code></dt>
<dd>
<div class="desc"><p>Retrieves the weights for the explainability metrics</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_weights_score_explainability(self, yaml_config_file, explainability_method):
    &#34;&#34;&#34;Retrieves the weights for the explainability metrics&#34;&#34;&#34;
    if (explainability_method is Trust.Explainability_method.LIME):
        self.load_weights_score_explainability_LIME(yaml_config_file)
    else:
        self.load_weights_score_explainability_TED(yaml_config_file) </code></pre>
</details>
</dd>
<dt id="trust.Trust_Weighted_Average.Trust_Weighted_Average.load_weights_score_explainability_LIME"><code class="name flex">
<span>def <span class="ident">load_weights_score_explainability_LIME</span></span>(<span>self, yaml_config_file)</span>
</code></dt>
<dd>
<div class="desc"><p>Retrieves the weights for the explainability metrics when using a LIME explainer</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_weights_score_explainability_LIME(self, yaml_config_file):
    &#34;&#34;&#34;Retrieves the weights for the explainability metrics when using a LIME explainer&#34;&#34;&#34;
    self.weight_expl_average_monotonicity = yaml_config_file[&#39;w.avg_parameters&#39;][&#39;explainability&#39;][&#39;expl_metrics_LIME&#39;][&#39;average_monotonicity_weight&#39;]
    self.weight_expl_average_faithfulness = yaml_config_file[&#39;w.avg_parameters&#39;][&#39;explainability&#39;][&#39;expl_metrics_LIME&#39;][&#39;average_faithfulness_weight&#39;] 

    if self.weight_expl_average_monotonicity + self.weight_expl_average_faithfulness != 1:
        raise ValueError(&#39;Explainability metrics\&#39; weights (LIME) do not add 1. Revise the configuration file&#39;)
    else:            
        self.score_expl_average_monotonicity = self.trust_instance.expl_average_monotonicity * self.weight_expl_average_monotonicity
        self.score_expl_average_faithfulness = self.trust_instance.expl_average_faithfulness * self.weight_expl_average_faithfulness

        self.score_explainability = (self.score_expl_average_monotonicity+self.score_expl_average_faithfulness)*self.weight_explainability</code></pre>
</details>
</dd>
<dt id="trust.Trust_Weighted_Average.Trust_Weighted_Average.load_weights_score_explainability_TED"><code class="name flex">
<span>def <span class="ident">load_weights_score_explainability_TED</span></span>(<span>self, yaml_config_file)</span>
</code></dt>
<dd>
<div class="desc"><p>Retrieves the weights for the explainability metrics when using a TED-enhanced classifier and explanations</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_weights_score_explainability_TED(self, yaml_config_file):
    &#34;&#34;&#34;Retrieves the weights for the explainability metrics when using a TED-enhanced classifier and explanations&#34;&#34;&#34;
    self.weight_expl_E_accuracy_score = yaml_config_file[&#39;w.avg_parameters&#39;][&#39;explainability&#39;][&#39;expl_metrics_TED&#39;][&#39;E_score_weight&#39;]

    if self.weight_expl_E_accuracy_score != 1:
        raise ValueError(&#39;Explainability metrics\&#39; weights (TED) do not add 1. Revise the configuration file&#39;)
    else:
        self.score_expl_E_accuracy_score = self.trust_instance.expl_E_accuracy*self.weight_expl_E_accuracy_score

        self.score_explainability = self.score_expl_E_accuracy_score*self.weight_explainability    </code></pre>
</details>
</dd>
<dt id="trust.Trust_Weighted_Average.Trust_Weighted_Average.load_weights_uncertainty"><code class="name flex">
<span>def <span class="ident">load_weights_uncertainty</span></span>(<span>self, yaml_config_file)</span>
</code></dt>
<dd>
<div class="desc"><p>Retrieves the weights for the uncertainty metrics</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_weights_uncertainty(self, yaml_config_file):
    &#34;&#34;&#34;Retrieves the weights for the uncertainty metrics&#34;&#34;&#34;
    weight_unc_brier_inv_score = yaml_config_file[&#39;w.avg_parameters&#39;][&#39;uncertainty&#39;][&#39;uncert_metrics&#39;][&#39;brier_inv_score_weight&#39;]
    weight_unc_expected_cal_inv_error = yaml_config_file[&#39;w.avg_parameters&#39;][&#39;uncertainty&#39;][&#39;uncert_metrics&#39;][&#39;expected_cal_inv_error_weight&#39;] 

    if weight_unc_brier_inv_score + weight_unc_expected_cal_inv_error != 1:
        raise ValueError(&#39;Uncertainty metrics\&#39; weights do not add 1. Revise the configuration file&#39;)
    else:
        return weight_unc_brier_inv_score, weight_unc_expected_cal_inv_error</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="trust" href="index.html">trust</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="trust.Trust_Weighted_Average.Trust_Weighted_Average" href="#trust.Trust_Weighted_Average.Trust_Weighted_Average">Trust_Weighted_Average</a></code></h4>
<ul class="">
<li><code><a title="trust.Trust_Weighted_Average.Trust_Weighted_Average.compute_score_fairness" href="#trust.Trust_Weighted_Average.Trust_Weighted_Average.compute_score_fairness">compute_score_fairness</a></code></li>
<li><code><a title="trust.Trust_Weighted_Average.Trust_Weighted_Average.compute_score_feedback" href="#trust.Trust_Weighted_Average.Trust_Weighted_Average.compute_score_feedback">compute_score_feedback</a></code></li>
<li><code><a title="trust.Trust_Weighted_Average.Trust_Weighted_Average.compute_score_performance" href="#trust.Trust_Weighted_Average.Trust_Weighted_Average.compute_score_performance">compute_score_performance</a></code></li>
<li><code><a title="trust.Trust_Weighted_Average.Trust_Weighted_Average.compute_score_robustness" href="#trust.Trust_Weighted_Average.Trust_Weighted_Average.compute_score_robustness">compute_score_robustness</a></code></li>
<li><code><a title="trust.Trust_Weighted_Average.Trust_Weighted_Average.compute_score_uncertainty" href="#trust.Trust_Weighted_Average.Trust_Weighted_Average.compute_score_uncertainty">compute_score_uncertainty</a></code></li>
<li><code><a title="trust.Trust_Weighted_Average.Trust_Weighted_Average.compute_trust" href="#trust.Trust_Weighted_Average.Trust_Weighted_Average.compute_trust">compute_trust</a></code></li>
<li><code><a title="trust.Trust_Weighted_Average.Trust_Weighted_Average.get_Trust_WA_as_JSON" href="#trust.Trust_Weighted_Average.Trust_Weighted_Average.get_Trust_WA_as_JSON">get_Trust_WA_as_JSON</a></code></li>
<li><code><a title="trust.Trust_Weighted_Average.Trust_Weighted_Average.load_weights_fairness" href="#trust.Trust_Weighted_Average.Trust_Weighted_Average.load_weights_fairness">load_weights_fairness</a></code></li>
<li><code><a title="trust.Trust_Weighted_Average.Trust_Weighted_Average.load_weights_performance" href="#trust.Trust_Weighted_Average.Trust_Weighted_Average.load_weights_performance">load_weights_performance</a></code></li>
<li><code><a title="trust.Trust_Weighted_Average.Trust_Weighted_Average.load_weights_robustness" href="#trust.Trust_Weighted_Average.Trust_Weighted_Average.load_weights_robustness">load_weights_robustness</a></code></li>
<li><code><a title="trust.Trust_Weighted_Average.Trust_Weighted_Average.load_weights_score_explainability" href="#trust.Trust_Weighted_Average.Trust_Weighted_Average.load_weights_score_explainability">load_weights_score_explainability</a></code></li>
<li><code><a title="trust.Trust_Weighted_Average.Trust_Weighted_Average.load_weights_score_explainability_LIME" href="#trust.Trust_Weighted_Average.Trust_Weighted_Average.load_weights_score_explainability_LIME">load_weights_score_explainability_LIME</a></code></li>
<li><code><a title="trust.Trust_Weighted_Average.Trust_Weighted_Average.load_weights_score_explainability_TED" href="#trust.Trust_Weighted_Average.Trust_Weighted_Average.load_weights_score_explainability_TED">load_weights_score_explainability_TED</a></code></li>
<li><code><a title="trust.Trust_Weighted_Average.Trust_Weighted_Average.load_weights_uncertainty" href="#trust.Trust_Weighted_Average.Trust_Weighted_Average.load_weights_uncertainty">load_weights_uncertainty</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>